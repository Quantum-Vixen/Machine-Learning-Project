{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from __future__ import annotations\n",
    "import itertools\n",
    "%matplotlib widget\n",
    "\n",
    "from typing import TypeAlias, Iterator, Iterable\n",
    "Vector: TypeAlias = np.ndarray  # A 1-D array\n",
    "Matrix: TypeAlias = np.ndarray  # A 2-D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerInitializationStrategy:\n",
    "    \"\"\"Parent class for initialization strategies of weights and biases in Layer.\"\"\"\n",
    "    def run(self, size_of_previous_layer: int, size_of_current_layer: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "        raise NotImplementedError(\"The 'run' method is implemented only in child classes\")\n",
    "\n",
    "class RandomUniform(LayerInitializationStrategy):\n",
    "    \"\"\"\n",
    "    Initialization strategy sampling weights and biases uniformly in a given interval.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    scale: float\n",
    "        The half-lenght of the interval [center-scale, center+scale] from which values are sampled.\n",
    "    center: float\n",
    "        The center of the aforementioned interval.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    run(self, size_of_previous_layer: int, size_of_current_layer: int) -> tuple[np.ndarray, np.ndarray]\n",
    "        Returns the tuple (random_weights, random_biases), where random_weights and random_biases are np.ndarrays of the appropriate shape.\n",
    "    \"\"\"\n",
    "    def __init__(self, scale_of_interval: float, center_of_interval: float = 0):\n",
    "        self.scale: float = scale_of_interval\n",
    "        self.center: float = center_of_interval\n",
    "        \n",
    "    \n",
    "    def run(self, size_of_previous_layer: int, size_of_current_layer: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "        random_weights: np.ndarray = np.random.uniform(\n",
    "            -self.scale + self.center, self.scale + self.center,\n",
    "            (size_of_previous_layer, size_of_current_layer)\n",
    "            )\n",
    "        random_biases: np.ndarray = np.random.uniform(\n",
    "            -self.scale + self.center, self.scale + self.center,\n",
    "            size_of_current_layer\n",
    "            )\n",
    "        return random_weights, random_biases\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Initialization: RandomUniform in [{self.center - self.scale}, {self.center + self.scale}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    \"\"\"Parent class for activation functions of neural nodes.\"\"\"\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError(\"The '__call__' method must be implemented in child classes\")\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError(\"The 'derivative' method must be implemented in child classes\")\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    \"\"\"Sigmoid activation function. f(x) = 1 / (1 + np.exp(-x))\"\"\"\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        sigmoid = self(x)  # Reuse the __call__ method to compute sigmoid\n",
    "        return sigmoid * (1 - sigmoid)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Sigmoid act. fun.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A Layer component of a NeuralNetwork.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    unit_number: int\n",
    "        The number of nodes/units in the Layer.\n",
    "    init_strat: LayerInitializationStrategy\n",
    "        The initialization strategy for the weights and biases of the Layer\n",
    "    activation_function: ActivationFunction\n",
    "        The function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, number_of_units: int,\n",
    "                 initialization_strategy: LayerInitializationStrategy,\n",
    "                 activation_function: ActivationFunction):\n",
    "        self.unit_number: int = number_of_units  # The number of nodes/units in the Layer.\n",
    "        self.init_strat: LayerInitializationStrategy = initialization_strategy\n",
    "        self.activation_function: ActivationFunction = activation_function\n",
    "\n",
    "        # The values computed by the units, based on the outputs of the previous layer. Stored for later backprop.\n",
    "        self.linear_output: Vector = None\n",
    "        self.output: Vector = None\n",
    "        \n",
    "        # The layer preceding the current one in the Neural Network. The NN should connect layers during initialization.\n",
    "        self.previous_layer: Layer = None\n",
    "        self.next_layer: Layer = None\n",
    "\n",
    "        # Weights and biases connecting the layer with the previous layer of the neural network.\n",
    "        self.weights: Matrix = None; self.biases: Vector = None\n",
    "\n",
    "        # A variable that needs to be computed from the delta of next layer in the\n",
    "        # Backprop TrainingAlgorithm\n",
    "        self.delta: Vector = None\n",
    "        \n",
    "\n",
    "    def initialize_weights(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the weights and biases of this Layer according to its init_strat.\n",
    "        \"\"\"\n",
    "        self.weights, self.biases = self.init_strat.run(self.previous_layer.unit_number, self.unit_number)\n",
    "\n",
    "    def compute_output(self):\n",
    "        \"\"\"\n",
    "        Computes the output of this layer as activation_function(np.dot(input, weights) + biases), where\n",
    "        the input is the output of the previous layer.\n",
    "        Stores the output as well as just the linear_output np.dot(input, weights) + biases, as it's useful in typical training algorithms.\n",
    "        \"\"\"\n",
    "        self.linear_output = np.dot(self.previous_layer.output, self.weights) + self.biases\n",
    "        self.output: Vector = self.activation_function(self.linear_output)\n",
    "        return self.output\n",
    "\n",
    "class InputLayer(Layer):\n",
    "    \"\"\"\n",
    "    The first Layer of a NeuralNetwork. It has no previous layer, and thus no weights and biases to connect it with.\n",
    "    Its activation function is the Identity.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    unit_number: int\n",
    "        The number of nodes/units in the Layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, number_of_units: int):\n",
    "        super().__init__(number_of_units, None, None)\n",
    "        # An input layer has no previous layer to connect to, so attributes referring to a previous layer are deleted.\n",
    "        del self.previous_layer, self.weights, self.biases, self.init_strat, self.activation_function\n",
    "    \n",
    "    def feed_input(self, value: Vector) -> None:\n",
    "        \"\"\"\n",
    "        Sets the input (which is also the output) of the InputLayer, and thus of the whole NeuralNetwork, to value.\n",
    "        \"\"\"\n",
    "        self.output: Vector = value\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        raise NotImplementedError(\"InputLayer does not require weight initialization.\")\n",
    "\n",
    "    def compute_output(self) -> Vector:\n",
    "        \"\"\"\n",
    "        Returns the output (which is also the input) of the InputLayer\n",
    "        \"\"\"\n",
    "        return self.output\n",
    "\n",
    "class HiddenLayer(Layer):\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Computes the delta of this layer from the delta of the next layer\n",
    "        as np.dot(self.next_layer.weights, self.next_layer.delta) * self.activation_function.derivative(self.linear_output)\n",
    "        \"\"\"\n",
    "        self.delta = np.dot(self.next_layer.weights, self.next_layer.delta) * self.activation_function.derivative(self.linear_output)\n",
    "\n",
    "class OutputLayer(Layer):\n",
    "    def __init__(self, number_of_units: int,\n",
    "                 initialization_strategy: LayerInitializationStrategy,\n",
    "                 activation_function: ActivationFunction):\n",
    "        super().__init__(number_of_units, initialization_strategy, activation_function)\n",
    "        del self.next_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListOfArrays:\n",
    "    \"\"\"\n",
    "    An utility class for defining element-wise operations on lists containing heteromorphic np.ndarrays.\n",
    "    Useful for conveniently manipulating network weights and biases in MLP-architecture NeuralNetworks.\n",
    "    \"\"\"\n",
    "    def __init__(self, arrays: list[np.ndarray]):\n",
    "        self.arrays: list[np.ndarray] = arrays\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ListOfArrays{(self.arrays)}\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.arrays[index]\n",
    "\n",
    "    def __setitem__(self, index, value):\n",
    "        self.arrays[index] = value\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if not isinstance(other, ListOfArrays): raise TypeError(\"Operand is not a ListOfArrays\")\n",
    "        return ListOfArrays([x + y for x, y in zip(self.arrays, other.arrays)])\n",
    "    \n",
    "    def __mul__(self, scalar: float):\n",
    "        return ListOfArrays([x * scalar for x in self.arrays])\n",
    "    \n",
    "    def __rmul__(self, scalar: float):\n",
    "        return self.__mul__(scalar)\n",
    "\n",
    "    def __truediv__(self, scalar: float):\n",
    "        return ListOfArrays([x / scalar for x in self.arrays])\n",
    "    \n",
    "    def __pow__(self, power: float):\n",
    "        return ListOfArrays([x**power for x in self.arrays])\n",
    "    \n",
    "    def sum(self) -> float:\n",
    "        return np.sum([np.sum(array) for array in self.arrays])\n",
    "    \n",
    "    def set_all_values_to(self, value: float) -> None:\n",
    "        for a in self.arrays:\n",
    "            a = value*np.ones(a.shape)\n",
    "\n",
    "\n",
    "class ListOfVectors(ListOfArrays):\n",
    "    \"\"\"\n",
    "    An utility class for convenient manipulation of lists of Vectors of different lengths.\n",
    "    Useful for network biases in MLP-architecture NeuralNetworks.\n",
    "    \"\"\"\n",
    "    def __init__(self, arrays: list[Vector]):\n",
    "        super().__init__(arrays)\n",
    "\n",
    "class ListOfMatrices(ListOfArrays):\n",
    "    \"\"\"\n",
    "    An utility class for convenient manipulation of lists of Matrices of different number of rows and cols.\n",
    "    Useful for network weights in MLP-architecture NeuralNetworks.\n",
    "    \"\"\"\n",
    "    pass\n",
    "    def __init__(self, arrays: list[Matrix]):\n",
    "        super().__init__(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineLearningModel:\n",
    "    pass\n",
    "\n",
    "class NeuralNetworkArchitecture:\n",
    "    \"\"\"\n",
    "    An utility class for storing information about the number of Layers and of units for each Layer of a NeuralNetwork, as well as other\n",
    "    useful data.\n",
    "\n",
    "    Used for an alternative constructor of NeuralNetwork when no fine-control for initialization is needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes_of_layers: list[int], activation_function: ActivationFunction, initialization_strategy: LayerInitializationStrategy):\n",
    "        self.sizes_of_layers: list[int] = sizes_of_layers\n",
    "        self.activation_function: ActivationFunction = activation_function\n",
    "        self.initialization_strategy: LayerInitializationStrategy = initialization_strategy\n",
    "    \n",
    "    def __str__(self):\n",
    "        network_shape: str = \"(\" + \", \".join(map(str, self.sizes_of_layers)) + \")\"\n",
    "        return f\"{network_shape}\"\n",
    "\n",
    "class NeuralNetwork(MachineLearningModel):\n",
    "    \"\"\"\n",
    "    A NeuralNetwork.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layers: list[Layer]\n",
    "        The list of Layer that make the NeuralNetwork.\n",
    "        Most NeuralNetwork methods work by invoking the appropriate Layer-level methods in the appropriate order.\n",
    "    input_layer: Layer\n",
    "        The first Layer of the NN\n",
    "    hidden_layers: list[Layer]\n",
    "        The list of non-first-nor-last Layers.\n",
    "    output_layer: Layer\n",
    "        The last Layer of the NN\n",
    "    layers_with_weights: list[Layer]\n",
    "        The list of all Layers, except the InputLayer.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: list[Layer]):\n",
    "        self.layers: list[Layer] = layers\n",
    "        # Maybe here I should ensure that layers are correctly typed (layers[0] should be an InputLayer, layers[-1] an OutputLayer, all other layers should be HiddenLayer).\n",
    "        self.input_layer: InputLayer = layers[0]; self.hidden_layers: list[HiddenLayer] = layers[1: -1]; self.output_layer: OutputLayer = layers[-1]\n",
    "        self.layers_with_weights: list[Layer] = self.layers[1: ]\n",
    "        self.connect_layers()\n",
    "        self.initialize_weights()\n",
    "\n",
    "    @classmethod\n",
    "    def FromArchitecture(cls, architecture: NeuralNetworkArchitecture):\n",
    "        sizes: list[int] = architecture.sizes_of_layers; act_fun = architecture.activation_function; init_strat = architecture.initialization_strategy\n",
    "        il: InputLayer = InputLayer(sizes[0])\n",
    "        hls: list[HiddenLayer] = [HiddenLayer(n, init_strat, act_fun) for n in sizes[1: -1]]\n",
    "        ol: OutputLayer = OutputLayer(sizes[-1], init_strat, act_fun)\n",
    "        layers: list[Layer] = [il] + hls + [ol]\n",
    "        return cls(layers)\n",
    "\n",
    "    def connect_layers(self) -> None:\n",
    "        for (i, layer) in enumerate(self.layers):\n",
    "            if not isinstance(layer, InputLayer): layer.previous_layer = self.layers[i - 1]\n",
    "            if not isinstance(layer, OutputLayer): layer.next_layer = self.layers[i + 1]\n",
    "\n",
    "    def initialize_weights(self) -> None:\n",
    "        for layer in self.layers_with_weights: layer.initialize_weights()\n",
    "    \n",
    "    def feed_input(self, value: np.ndarray) -> None:\n",
    "        self.input_layer.feed_input(value)\n",
    "\n",
    "    def activate_network(self) -> np.ndarray:\n",
    "        for i in range(len(self.layers)): self.layers[i].compute_output()\n",
    "        return self.output_layer.output\n",
    "    \n",
    "    def compute_output(self, value: np.ndarray) -> np.ndarray:\n",
    "        self.feed_input(value)\n",
    "        return self.activate_network()\n",
    "    \n",
    "    def backward(self) -> None:\n",
    "        for l in reversed(self.hidden_layers):\n",
    "            l.backward()\n",
    "    \n",
    "    def compute_multiple_outputs(self, x_data: pd.DataFrame | np.ndarray) -> np.ndarray[np.ndarray]:\n",
    "        if isinstance(x_data, pd.DataFrame): x_data = x_data.to_numpy()\n",
    "        outputs = np.array(\n",
    "            [\n",
    "                self.compute_output(x_data[i]) for i in range(len(x_data))\n",
    "            ]\n",
    "        )\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizationTerm:\n",
    "    def set_network(self, network: NeuralNetwork) -> None:\n",
    "        self.network = network\n",
    "\n",
    "    def __call__(self) -> float:\n",
    "        pass\n",
    "\n",
    "    def gradient(self) -> tuple[ListOfArrays, ListOfArrays]:\n",
    "        pass\n",
    "\n",
    "class NoRegularization(RegularizationTerm):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self) -> float:\n",
    "        return 0\n",
    "    \n",
    "    def gradient(self) -> tuple[ListOfArrays, ListOfArrays]:\n",
    "        layers = self.network.layers_with_weights\n",
    "        return ListOfArrays([np.zeros_like(l.weights) for l in layers]), ListOfArrays([np.zeros_like(l.biases) for l in layers])\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"No reg.\"\n",
    "\n",
    "class Tikhonov(RegularizationTerm):\n",
    "    \"\"\"\n",
    "    A regularization penalty term of the form constant*(sum of squares of weights and biases).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    penalty: float\n",
    "        The constant factor multiplying the sum of squares.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    penalty: float\n",
    "        The constant factor multiplying the sum of squares.\n",
    "    network: NeuralNetwork\n",
    "        The NeuralNetwork that weights and biases are read from.\n",
    "    \"\"\"\n",
    "    def __init__(self, penalty: float):\n",
    "        self.penalty: float = penalty\n",
    "        self.network: NeuralNetwork = None\n",
    "\n",
    "    def __call__(self) -> float:\n",
    "        layers: list[Layer] = self.network.layers_with_weights\n",
    "        weights_term = np.sum([np.sum(layer.weigths**2) for layer in layers])  # The sum of squares of all the weights in the NN.\n",
    "        biases_term = np.sum([np.sum(layer.biases**2) for layer in layers])\n",
    "        return self.penalty * (weights_term + biases_term) / 2\n",
    "\n",
    "    def gradient(self) -> tuple[ListOfArrays, ListOfArrays]:\n",
    "        layers: list[Layer] = self.network.layers_with_weights\n",
    "        gradient_on_weights: ListOfArrays = ListOfArrays([-self.penalty * l.weights for l in layers])\n",
    "        gradient_on_biases: ListOfArrays = ListOfArrays([-self.penalty * l.biases for l in layers])\n",
    "        return gradient_on_weights, gradient_on_biases\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Tikhonov({self.penalty})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumRule:\n",
    "    pass\n",
    "\n",
    "class ClassicalMomentum(MomentumRule):\n",
    "    def __init__(self, decay_factor: float):\n",
    "        self.decay_factor = decay_factor\n",
    "\n",
    "class NesterovMomentum(MomentumRule):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingCondition:\n",
    "    def __init__(self):\n",
    "        self.alg: TrainingAlgorithm = None\n",
    "    \n",
    "    def set_alg(self, alg: TrainingAlgorithm) -> None:\n",
    "        self.alg = alg\n",
    "\n",
    "    @property\n",
    "    def is_satisfied(self) -> bool:\n",
    "        pass\n",
    "\n",
    "class ThresholdOnTrainingError(StoppingCondition):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    threshold: float\n",
    "    patience: int\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold: float, patience: int):\n",
    "        super().__init__()\n",
    "        self.threshold: float = threshold\n",
    "        self.patience: int = patience\n",
    "    \n",
    "    @property\n",
    "    def is_satisfied(self) -> bool:\n",
    "        current_training_error: float = self.alg.current_tr_err\n",
    "        if current_training_error < self.threshold:\n",
    "            self.consecutive_epochs += 1\n",
    "            return self.consecutive_epochs > self.patience\n",
    "        else:\n",
    "            self.consecutive_epochs = 0\n",
    "            return False\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"TR Err threshold: {self.threshold}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorFunction:\n",
    "    def __call__(self, y_data: np.ndarray, y_predicted: np.ndarray) -> float:\n",
    "        pass\n",
    "    \n",
    "    def simple_gradient(self, y_data: np.ndarray, y_predicted: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "class MSE(ErrorFunction):\n",
    "    def __call__(self, y_data: np.ndarray, y_predicted: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Returns the average over the dataset of the square euclidean distance between the training outputs and the predictions.\n",
    "        \"\"\"\n",
    "        num_patterns = 1 if y_data.ndim == 1 else len(y_data)\n",
    "        return 0.5 * np.sum((y_data - y_predicted)**2) / num_patterns\n",
    "    \n",
    "    def simple_gradient(self, y_data: np.ndarray, y_predicted: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns y_data - y_predicted. It is meant to be used on a single pattern at a time, during backpropagation.\n",
    "        \"\"\"\n",
    "        return (y_data - y_predicted)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"MSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, x_data: Matrix, y_data: Matrix):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with input (x_data) and output (y_data).\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(x_data, pd.DataFrame): x_data = x_data.to_numpy()\n",
    "        if isinstance(y_data, pd.DataFrame): y_data = y_data.to_numpy()\n",
    "        \n",
    "        if x_data.ndim == 1 or y_data.ndim == 1:\n",
    "            raise ValueError(f\"x_data and y_data should be matrices, where each row represents a pattern and each column a feature, but got arguments of shape {x_data.shape} and {y_data.shape}\")\n",
    "\n",
    "        self.x: Matrix = x_data\n",
    "        self.y: Matrix = y_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of patterns in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves the input-output pair at the specified index.\n",
    "        \"\"\"\n",
    "        if isinstance(index, slice):\n",
    "            return Dataset(self.x[index], self.y[index])\n",
    "        # This should be refactored in the future. The behaviour should be the same as numpy, regardless of index type.\n",
    "        elif isinstance(index, list) or isinstance(index, np.ndarray):\n",
    "            return Dataset(self.x[index], self.y[index])\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def shuffle(self) -> None:\n",
    "        indices: np.ndarray = np.arange(len(self))\n",
    "        np.random.shuffle(indices)\n",
    "        self.x = self.x[indices, :]\n",
    "        self.y = self.y[indices, :]\n",
    "    \n",
    "    def split(self, fraction: float, shuffle: bool = True) -> tuple[Dataset, Dataset]:\n",
    "        \"\"\"\n",
    "        Returns two datasets, one with fraction*N data and the other with (1-fraction)*N.\n",
    "        \"\"\"\n",
    "        indices: np.ndarray = np.arange(len(self))\n",
    "        \n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        splitting_number: int = int(fraction * len(self))\n",
    "        indices_1: np.ndarray = indices[:splitting_number]\n",
    "        \n",
    "        indices_2: np.ndarray = indices[splitting_number:]\n",
    "        ds_1: Dataset = Dataset(self.x[indices_1, :], self.y[indices_1, :])\n",
    "        ds_2: Dataset = Dataset(self.x[indices_2, :], self.y[indices_2, :])\n",
    "        return ds_1, ds_2\n",
    "    \n",
    "    @classmethod\n",
    "    def concatenate(cls, datasets: Iterable[Dataset]) -> Dataset:\n",
    "        x = np.concatenate([ds.x for ds in datasets])\n",
    "        y = np.concatenate([ds.y for ds in datasets])\n",
    "        return Dataset(x, y)\n",
    "\n",
    "\n",
    "class DataManager:\n",
    "    \"\"\"An auxiliary class for extracting minibatches from a Dataset.\"\"\"\n",
    "    def __init__(self, dataset: Dataset, minibatch_size: int = None, shuffle: bool = True):\n",
    "        self.dataset: Dataset = dataset\n",
    "        self.minibatch_size: int = minibatch_size or len(dataset)\n",
    "        self.shuffle: bool = shuffle\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        An iterator yielding minibatches.\n",
    "        \"\"\"\n",
    "        indices = np.arange(len(self.dataset))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        for start in range(0, len(indices), self.minibatch_size):\n",
    "            minibatch_indices = indices[start:start + self.minibatch_size]\n",
    "            minibatch_x = self.dataset.x[minibatch_indices, :]\n",
    "            minibatch_y = self.dataset.y[minibatch_indices, :]\n",
    "            yield minibatch_x, minibatch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keys:\n",
    "    ARCHITECTURE = \"architecture\"\n",
    "    TR_ALGORITHM = \"training algorithm\"\n",
    "    HYP_COMB = \"hyperparameters combination\"\n",
    "    TR_ERROR = \"training error\"\n",
    "    TR_CURVE = \"training curve\"\n",
    "    VL_ERROR = \"validation error\"\n",
    "    VL_CURVE = \"validation curve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingAlgorithm:\n",
    "    def __init__(self, x_train: pd.DataFrame, y_train: pd.DataFrame, network: NeuralNetwork):\n",
    "        self.network: NeuralNetwork = network\n",
    "\n",
    "        if isinstance(x_train, pd.DataFrame): x_train = x_train.to_numpy()\n",
    "        if isinstance(y_train, pd.DataFrame): y_train = y_train.to_numpy()\n",
    "\n",
    "        self.training_set = Dataset(x_train, y_train)\n",
    "\n",
    "        self.current_tr_err: float = float('inf')\n",
    "\n",
    "        self.history: dict[list] = {Keys.TR_ERROR: []}\n",
    "        self.cache: dict = {}\n",
    "\n",
    "class Backprop(TrainingAlgorithm):\n",
    "    def __init__(self, x_train: pd.DataFrame, y_train: pd.DataFrame, network: NeuralNetwork,\n",
    "                 learning_rate: float,\n",
    "                 error_function: ErrorFunction,\n",
    "                 stopping_condition: StoppingCondition,\n",
    "                 regularization_term: RegularizationTerm = None,\n",
    "                 minibatch_size: int = None\n",
    "                 ):\n",
    "        super().__init__(x_train, y_train, network)\n",
    "\n",
    "        self.learning_rate: float = learning_rate\n",
    "\n",
    "        self.err_fun: ErrorFunction = error_function\n",
    "        \n",
    "        self.stop_cond: StoppingCondition = stopping_condition\n",
    "        self.stop_cond.set_alg(self)\n",
    "\n",
    "        self.weights_gradient: ListOfMatrices = ListOfMatrices([np.zeros_like(l.weights) for l in self.network.layers_with_weights])\n",
    "        self.biases_gradient: ListOfVectors = ListOfVectors([np.zeros_like(l.biases) for l in self.network.layers_with_weights])\n",
    "\n",
    "        self.regularization_term: RegularizationTerm = regularization_term or NoRegularization()\n",
    "        self.regularization_term.set_network(self.network)\n",
    "\n",
    "        self.minibatch_size: int = minibatch_size or len(self.training_set)\n",
    "        self.minibatch_generator: DataManager = DataManager(self.training_set, self.minibatch_size,\n",
    "                                                            shuffle = (self.minibatch_size != len(self.training_set))\n",
    "                                                            )\n",
    "        \n",
    "        \n",
    "        self.current_mb_size: int = None\n",
    "        self.current_mb_x: Matrix = None; self.current_mb_y: Matrix = None\n",
    "        \n",
    "    \n",
    "    def run(self, max_epochs: int) -> None:\n",
    "        epoch: int = 0\n",
    "        while epoch < max_epochs:\n",
    "            epoch += 1\n",
    "\n",
    "            for minibatch_x, minibatch_y in self.minibatch_generator:\n",
    "                self.update_minibatch_metadata(minibatch_x, minibatch_y)\n",
    "                self.update_gradients()\n",
    "                self.update_network_parameters()\n",
    "            \n",
    "            self.compute_training_error()\n",
    "            if self.stop_cond.is_satisfied: break\n",
    "\n",
    "    def update_minibatch_metadata(self, minibatch_x: Matrix, minibatch_y: Matrix):\n",
    "        self.current_mb_x = minibatch_x; self.current_mb_y = minibatch_y\n",
    "        self.current_mb_size = len(minibatch_x)\n",
    "    \n",
    "    def update_gradients(self):\n",
    "        self.reset_gradients()\n",
    "\n",
    "        for x, y in zip(self.current_mb_x, self.current_mb_y):\n",
    "            predicted_y = self.network.compute_output(x)\n",
    "            out_l = self.network.output_layer\n",
    "            out_l.delta = self.err_fun.simple_gradient(y, predicted_y)*out_l.activation_function.derivative(out_l.linear_output)\n",
    "            self.network.backward()\n",
    "            self.weights_gradient += ListOfMatrices([np.outer(l.previous_layer.output, l.delta) for l in self.network.layers_with_weights])\n",
    "            self.biases_gradient += ListOfVectors([l.delta for l in self.network.layers_with_weights])\n",
    "        self.weights_gradient /= self.current_mb_size; self.biases_gradient /= self.current_mb_size\n",
    "\n",
    "        self.add_regul_contribution()\n",
    "        self.add_momentum_contribution()\n",
    "    \n",
    "    def reset_gradients(self) -> None:\n",
    "        self.weights_gradient.set_all_values_to(0)\n",
    "        self.biases_gradient.set_all_values_to(0)\n",
    "    \n",
    "    def add_regul_contribution(self) -> None:\n",
    "        contribution_to_w, contribution_to_b = self.regularization_term.gradient()\n",
    "        self.weights_gradient += contribution_to_w; self.biases_gradient += contribution_to_b\n",
    "\n",
    "    def add_momentum_contribution(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def update_network_parameters(self) -> None:\n",
    "        factor = self.learning_rate * self.current_mb_size / len(self.training_set)\n",
    "        \n",
    "        for i, l in enumerate(self.network.layers_with_weights):\n",
    "            l.weights += factor * self.weights_gradient[i]\n",
    "            l.biases += factor * self.biases_gradient[i]\n",
    "\n",
    "    def compute_training_error(self) -> None:\n",
    "        y_prediction = self.network.compute_multiple_outputs(self.training_set.x)\n",
    "        self.current_tr_err = self.err_fun(self.training_set.y, y_prediction)\n",
    "        self.history[Keys.TR_ERROR] += [self.current_tr_err]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def check_tr_hyperparams(training_hyperparameters: dict, algorithm_class: type[TrainingAlgorithm]):\n",
    "    signature = inspect.signature(algorithm_class.__init__)\n",
    "    expected_hyparams = signature.parameters\n",
    "    for hyparam_name in training_hyperparameters.keys():\n",
    "        if hyparam_name not in expected_hyparams:\n",
    "            raise ValueError(f\"Unexpected parameter: {hyparam_name}\")\n",
    "\n",
    "\n",
    "\n",
    "def compute_network_error(network: NeuralNetwork, dataset: Dataset, error_function: ErrorFunction) -> float:\n",
    "    y_prediction = network.compute_multiple_outputs(dataset.x)\n",
    "    return error_function(dataset.y, y_prediction)\n",
    "\n",
    "\n",
    "\n",
    "class HyperparameterCombination:\n",
    "    def __init__(self,\n",
    "                 architecture: NeuralNetworkArchitecture,\n",
    "                 algorithm_class: type[TrainingAlgorithm],\n",
    "                 training_hyperparameters: dict[str, object],\n",
    "                 ensure_hyperparameter_compatibility: bool = True):\n",
    "        self.architecture: NeuralNetworkArchitecture = architecture\n",
    "        self.alg_cls: type[TrainingAlgorithm] = algorithm_class\n",
    "        self.tr_hyp: dict[str, object] = training_hyperparameters\n",
    "\n",
    "        if ensure_hyperparameter_compatibility:\n",
    "            check_tr_hyperparams(self.tr_hyp, algorithm_class)\n",
    "    \n",
    "    def keys(self) -> list[str]:\n",
    "        return [Keys.ARCHITECTURE, Keys.TR_ALGORITHM] + list(self.tr_hyp.keys())\n",
    "    \n",
    "    def training_hyperparameters_keys(self) -> list[str]:\n",
    "        return self.tr_hyp.keys()\n",
    "    \n",
    "    def as_dict(self) -> dict[str, object]:\n",
    "        return {Keys.ARCHITECTURE: str(self.architecture),\n",
    "                Keys.TR_ALGORITHM: str(self.alg_cls.__name__),\n",
    "                **{k: str(v) for k, v in self.tr_hyp.items()}\n",
    "                }\n",
    "\n",
    "\n",
    "class HyperparameterGrid:\n",
    "    def __init__(self,\n",
    "                 list_of_architectures: list[NeuralNetworkArchitecture],\n",
    "                 algorithm_class: type[TrainingAlgorithm],\n",
    "                 lists_of_training_hyperparameters: dict[str, list[object]]\n",
    "                 ):\n",
    "        self.architectures: list[NeuralNetworkArchitecture] = list_of_architectures\n",
    "        self.alg_cls: type[TrainingAlgorithm] = algorithm_class\n",
    "        self.tr_hyp_lists: dict[str, list[object]] = lists_of_training_hyperparameters\n",
    "\n",
    "        self._list: list[HyperparameterCombination] = None  # Store the result of the to_list() method the first time it's called\n",
    "\n",
    "        check_tr_hyperparams(self.tr_hyp_lists, algorithm_class)\n",
    "    \n",
    "    def to_list(self) -> list[HyperparameterCombination]:\n",
    "        if self._list is None:\n",
    "            self._list = []\n",
    "            for arc in self.architectures:\n",
    "                for tr_hyp_values in itertools.product(*self.tr_hyp_lists.values()):\n",
    "                    tr_hyp: dict[str, object] = dict(zip(self.tr_hyp_lists.keys(), tr_hyp_values))\n",
    "                    self._list.append(\n",
    "                        HyperparameterCombination(arc, self.alg_cls, tr_hyp)\n",
    "                    )\n",
    "        return self._list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parenthetical_uncertainty_format(number: float, uncertainty: float, uncertainty_digits: int = 2) -> str:\n",
    "    import math\n",
    "    num_exponent = math.floor(math.log10(abs(number))) if number != 0 else 0\n",
    "    unc_exponent = math.floor(math.log10(abs(uncertainty))) if uncertainty != 0 else 0\n",
    "\n",
    "    n = num_exponent - unc_exponent + 1  # Number of significant digits\n",
    "    if n <= 0: return f\"{number} +- {uncertainty}\"\n",
    "    scaled_number = number * 10 ** -num_exponent\n",
    "    num_string = f\"{scaled_number:.{n - 1 + uncertainty_digits - 1}f}\"\n",
    "\n",
    "    scaled_uncertainty = uncertainty * 10 ** -unc_exponent\n",
    "    unc_string = f\"{scaled_uncertainty:.{uncertainty_digits - 1}f}\".replace('.', '')\n",
    "\n",
    "    return f\"{num_string}({unc_string})e{num_exponent}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluationMethod:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "class SelectionMethod(ModelEvaluationMethod):\n",
    "    pass\n",
    "\n",
    "class HoldOutSelection(SelectionMethod):\n",
    "    def __init__(self, training_set: Dataset, validation_set: Dataset,\n",
    "                 hyperparameter_combinations: list[HyperparameterCombination],\n",
    "                 risk_function: ErrorFunction\n",
    "                 ):\n",
    "        self.tr_set: Dataset = training_set\n",
    "        self.vl_set: Dataset = validation_set\n",
    "        self.hyp_combs: list[HyperparameterCombination] = hyperparameter_combinations\n",
    "        self.risk_fun: ErrorFunction = risk_function\n",
    "\n",
    "        self.results: dict[HyperparameterCombination, float] = {\n",
    "            hyp_comb: None for hyp_comb in self.hyp_combs\n",
    "        }  # Storage for the validation error of each combination of hyperparams.\n",
    "\n",
    "        self.tr_err_curves: dict[HyperparameterCombination, list[float]] = {\n",
    "            hyp_comb: [] for hyp_comb in self.hyp_combs\n",
    "        }\n",
    "\n",
    "        self.vl_err_curves: dict[HyperparameterCombination, list[float]] = {\n",
    "            hyp_comb: [] for hyp_comb in self.hyp_combs\n",
    "        }\n",
    "\n",
    "        \n",
    "\n",
    "    def run(self, max_epochs: int, trace_error = False) -> None:\n",
    "        \n",
    "\n",
    "        def process_single_hyp_comb(hyp_comb: HyperparameterCombination) -> dict[str, object]:\n",
    "\n",
    "            out: dict[str, object] = {}\n",
    "            arc, alg_cls, tr_hyp = hyp_comb.architecture, hyp_comb.alg_cls, hyp_comb.tr_hyp\n",
    "\n",
    "            network = NeuralNetwork.FromArchitecture(arc)\n",
    "            alg: TrainingAlgorithm = alg_cls(self.tr_set.x, self.tr_set.y, network, **tr_hyp)\n",
    "\n",
    "            if trace_error:\n",
    "                out[Keys.VL_CURVE] = []\n",
    "                for _ in range(max_epochs):\n",
    "                    alg.run(1)\n",
    "                    vl_err: float = compute_network_error(network, self.vl_set, self.risk_fun)\n",
    "                    out[Keys.VL_CURVE].append(vl_err)\n",
    "                out[Keys.TR_CURVE] = alg.history[Keys.TR_ERROR]\n",
    "            else:\n",
    "                alg.run(max_epochs)\n",
    "                vl_err: float = compute_network_error(network, self.vl_set, self.risk_fun)\n",
    "            out[Keys.VL_ERROR] = vl_err\n",
    "            \n",
    "            return out\n",
    "\n",
    "        single_hyp_comb_results: list[dict] = Parallel(n_jobs=-1)(\n",
    "            delayed(process_single_hyp_comb)(hyp_comb) for hyp_comb in self.hyp_combs\n",
    "        )\n",
    "        for hyp_comb, hcr in zip(self.hyp_combs, single_hyp_comb_results):\n",
    "\n",
    "            self.results[hyp_comb] = hcr[Keys.VL_ERROR]\n",
    "            if Keys.VL_CURVE in hcr.keys(): self.vl_err_curves[hyp_comb] = hcr[Keys.VL_CURVE]\n",
    "            if Keys.TR_CURVE in hcr.keys(): self.tr_err_curves[hyp_comb] = hcr[Keys.TR_CURVE]\n",
    "    \n",
    "\n",
    "\n",
    "    def results_dataframe(self) -> pd.DataFrame:\n",
    "        out: list[dict] = []\n",
    "        for k, v in self.results.items():\n",
    "            dic = k.as_dict()\n",
    "            dic.update({Keys.VL_ERROR: v})\n",
    "            out.append(dic)\n",
    "        return pd.DataFrame(out)\n",
    "\n",
    "class KFoldCrossValidation(SelectionMethod):\n",
    "    def __init__(self, dataset: Dataset, number_of_folds: int,\n",
    "                 hyperparameter_combinations: list[HyperparameterCombination],\n",
    "                 risk_function: ErrorFunction,\n",
    "                 shuffle_data: bool = True):\n",
    "        self.dataset: Dataset = dataset\n",
    "        self.n_folds: int = number_of_folds; self.fold_length: int = int(len(self.dataset) / self.n_folds)\n",
    "        self.hyp_combs: list[HyperparameterCombination] = hyperparameter_combinations\n",
    "        self.risk_fun: ErrorFunction = risk_function\n",
    "\n",
    "        if shuffle_data: self.dataset.shuffle()\n",
    "\n",
    "        effective_end = (len(self.dataset) // self.fold_length) * self.fold_length\n",
    "        starts = range(0, effective_end, self.fold_length)\n",
    "        self.fold_sets: list[Dataset] = [self.dataset[start:start + self.fold_length] for start in starts]\n",
    "\n",
    "        self.results: dict[HyperparameterCombination, Vector] = {\n",
    "            hyp_comb: np.empty(self.n_folds) for hyp_comb in self.hyp_combs\n",
    "        }\n",
    "    \n",
    "    def run(self, max_epochs: int):\n",
    "        for k, fold_set in enumerate(self.fold_sets):\n",
    "            vl_set: Dataset = fold_set\n",
    "            tr_set: Dataset = Dataset.concatenate([other_fold_set for other_fold_set in self.fold_sets if other_fold_set != fold_set])\n",
    "\n",
    "            hold_out: HoldOutSelection = HoldOutSelection(tr_set, vl_set, self.hyp_combs, self.risk_fun)\n",
    "            hold_out.run(max_epochs, trace_error = False)\n",
    "            \n",
    "            for hyp_comb in self.hyp_combs:\n",
    "                self.results[hyp_comb][k] = hold_out.results[hyp_comb]\n",
    "    \n",
    "    def results_dataframe(self) -> pd.DataFrame:\n",
    "        out: list[dict] = []\n",
    "        for k, vector in self.results.items():\n",
    "            dic = k.as_dict()\n",
    "            dic.update({Keys.VL_ERROR: parenthetical_uncertainty_format(vector.mean(), vector.std(ddof = 1))})\n",
    "            out.append(dic)\n",
    "        return pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams: dict[list] = {\n",
    "    'learning_rate': [10.9],\n",
    "    'error_function': [MSE()],\n",
    "    'stopping_condition': [ThresholdOnTrainingError(0.0001, 10)],\n",
    "    'regularization_term': [Tikhonov(5e-5),\n",
    "                            Tikhonov(5e-7),\n",
    "                            NoRegularization()\n",
    "                            ],\n",
    "    'minibatch_size': [2]\n",
    "}\n",
    "\n",
    "list_of_architectures = [\n",
    "        NeuralNetworkArchitecture([17, 4, 2], Sigmoid(), RandomUniform(0.3)),\n",
    "        NeuralNetworkArchitecture([17, 5, 2], Sigmoid(), RandomUniform(0.3))\n",
    "    ]\n",
    "\n",
    "grid = HyperparameterGrid(list_of_architectures, Backprop, hyperparams)\n",
    "\n",
    "l = grid.to_list()\n",
    "\n",
    "\n",
    "monk2train_path = os.path.join('..', 'monk+s+problems', 'monks-2.train')\n",
    "df = pd.read_csv(monk2train_path, sep = ' ', skipinitialspace= True, names= ['class', 'a1','a2','a3','a4','a5','a6', 'ID'])\n",
    "raw_x = df.loc[:, 'a1':'a6']; raw_y = df.loc[:, 'class']\n",
    "x = pd.get_dummies(raw_x, columns = raw_x.columns).astype(int); y = pd.get_dummies(raw_y).astype(int)\n",
    "\n",
    "\n",
    "dataset = Dataset(x, y)\n",
    "tr_set, vl_set = dataset.split(0.7, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>architecture</th>\n",
       "      <th>training algorithm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>error_function</th>\n",
       "      <th>stopping_condition</th>\n",
       "      <th>regularization_term</th>\n",
       "      <th>minibatch_size</th>\n",
       "      <th>validation error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(17, 4, 2)</td>\n",
       "      <td>Backprop</td>\n",
       "      <td>10.9</td>\n",
       "      <td>MSE</td>\n",
       "      <td>TR Err threshold: 0.0001</td>\n",
       "      <td>Tikhonov(5e-05)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(17, 4, 2)</td>\n",
       "      <td>Backprop</td>\n",
       "      <td>10.9</td>\n",
       "      <td>MSE</td>\n",
       "      <td>TR Err threshold: 0.0001</td>\n",
       "      <td>Tikhonov(5e-07)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(17, 4, 2)</td>\n",
       "      <td>Backprop</td>\n",
       "      <td>10.9</td>\n",
       "      <td>MSE</td>\n",
       "      <td>TR Err threshold: 0.0001</td>\n",
       "      <td>No reg.</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(17, 5, 2)</td>\n",
       "      <td>Backprop</td>\n",
       "      <td>10.9</td>\n",
       "      <td>MSE</td>\n",
       "      <td>TR Err threshold: 0.0001</td>\n",
       "      <td>Tikhonov(5e-05)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(17, 5, 2)</td>\n",
       "      <td>Backprop</td>\n",
       "      <td>10.9</td>\n",
       "      <td>MSE</td>\n",
       "      <td>TR Err threshold: 0.0001</td>\n",
       "      <td>Tikhonov(5e-07)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(17, 5, 2)</td>\n",
       "      <td>Backprop</td>\n",
       "      <td>10.9</td>\n",
       "      <td>MSE</td>\n",
       "      <td>TR Err threshold: 0.0001</td>\n",
       "      <td>No reg.</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  architecture training algorithm learning_rate error_function  \\\n",
       "0   (17, 4, 2)           Backprop          10.9            MSE   \n",
       "1   (17, 4, 2)           Backprop          10.9            MSE   \n",
       "2   (17, 4, 2)           Backprop          10.9            MSE   \n",
       "3   (17, 5, 2)           Backprop          10.9            MSE   \n",
       "4   (17, 5, 2)           Backprop          10.9            MSE   \n",
       "5   (17, 5, 2)           Backprop          10.9            MSE   \n",
       "\n",
       "         stopping_condition regularization_term minibatch_size  \\\n",
       "0  TR Err threshold: 0.0001     Tikhonov(5e-05)              2   \n",
       "1  TR Err threshold: 0.0001     Tikhonov(5e-07)              2   \n",
       "2  TR Err threshold: 0.0001             No reg.              2   \n",
       "3  TR Err threshold: 0.0001     Tikhonov(5e-05)              2   \n",
       "4  TR Err threshold: 0.0001     Tikhonov(5e-07)              2   \n",
       "5  TR Err threshold: 0.0001             No reg.              2   \n",
       "\n",
       "   validation error  \n",
       "0          0.002723  \n",
       "1          0.000650  \n",
       "2          0.000665  \n",
       "3          0.002433  \n",
       "4          0.000575  \n",
       "5          0.000460  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl_method = HoldOutSelection(tr_set, vl_set,\n",
    "                             grid.to_list(),\n",
    "                             MSE())\n",
    "\n",
    "vl_method.run(500, trace_error = False)\n",
    "vl_method.results_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>architecture</th>\n",
       "      <th>training algorithm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>error_function</th>\n",
       "      <th>stopping_condition</th>\n",
       "      <th>regularization_term</th>\n",
       "      <th>minibatch_size</th>\n",
       "      <th>validation error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(17, 4, 2)</td>\n",
       "      <td>Backprop</td>\n",
       "      <td>10.9</td>\n",
       "      <td>MSE</td>\n",
       "      <td>TR Err threshold: 0.0001</td>\n",
       "      <td>Tikhonov(5e-05)</td>\n",
       "      <td>2</td>\n",
       "      <td>2.53(42)e-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(17, 4, 2)</td>\n",
       "      <td>Backprop</td>\n",
       "      <td>10.9</td>\n",
       "      <td>MSE</td>\n",
       "      <td>TR Err threshold: 0.0001</td>\n",
       "      <td>Tikhonov(5e-07)</td>\n",
       "      <td>2</td>\n",
       "      <td>6.5(25)e-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(17, 4, 2)</td>\n",
       "      <td>Backprop</td>\n",
       "      <td>10.9</td>\n",
       "      <td>MSE</td>\n",
       "      <td>TR Err threshold: 0.0001</td>\n",
       "      <td>No reg.</td>\n",
       "      <td>2</td>\n",
       "      <td>6.6(26)e-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(17, 5, 2)</td>\n",
       "      <td>Backprop</td>\n",
       "      <td>10.9</td>\n",
       "      <td>MSE</td>\n",
       "      <td>TR Err threshold: 0.0001</td>\n",
       "      <td>Tikhonov(5e-05)</td>\n",
       "      <td>2</td>\n",
       "      <td>2.64(63)e-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(17, 5, 2)</td>\n",
       "      <td>Backprop</td>\n",
       "      <td>10.9</td>\n",
       "      <td>MSE</td>\n",
       "      <td>TR Err threshold: 0.0001</td>\n",
       "      <td>Tikhonov(5e-07)</td>\n",
       "      <td>2</td>\n",
       "      <td>5.9(22)e-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(17, 5, 2)</td>\n",
       "      <td>Backprop</td>\n",
       "      <td>10.9</td>\n",
       "      <td>MSE</td>\n",
       "      <td>TR Err threshold: 0.0001</td>\n",
       "      <td>No reg.</td>\n",
       "      <td>2</td>\n",
       "      <td>6.1(18)e-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  architecture training algorithm learning_rate error_function  \\\n",
       "0   (17, 4, 2)           Backprop          10.9            MSE   \n",
       "1   (17, 4, 2)           Backprop          10.9            MSE   \n",
       "2   (17, 4, 2)           Backprop          10.9            MSE   \n",
       "3   (17, 5, 2)           Backprop          10.9            MSE   \n",
       "4   (17, 5, 2)           Backprop          10.9            MSE   \n",
       "5   (17, 5, 2)           Backprop          10.9            MSE   \n",
       "\n",
       "         stopping_condition regularization_term minibatch_size  \\\n",
       "0  TR Err threshold: 0.0001     Tikhonov(5e-05)              2   \n",
       "1  TR Err threshold: 0.0001     Tikhonov(5e-07)              2   \n",
       "2  TR Err threshold: 0.0001             No reg.              2   \n",
       "3  TR Err threshold: 0.0001     Tikhonov(5e-05)              2   \n",
       "4  TR Err threshold: 0.0001     Tikhonov(5e-07)              2   \n",
       "5  TR Err threshold: 0.0001             No reg.              2   \n",
       "\n",
       "  validation error  \n",
       "0      2.53(42)e-3  \n",
       "1       6.5(25)e-4  \n",
       "2       6.6(26)e-4  \n",
       "3      2.64(63)e-3  \n",
       "4       5.9(22)e-4  \n",
       "5       6.1(18)e-4  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl_method = KFoldCrossValidation(dataset, 12,\n",
    "                                 grid.to_list(), MSE(), True)\n",
    "vl_method.run(500)\n",
    "vl_method.results_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2(11)e-1'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parenthetical_uncertainty_format(0.12, 0.112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KFoldCrossValidation' object has no attribute 'tr_err_curves'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      3\u001b[0m hyp_comb \u001b[38;5;241m=\u001b[39m vl_method\u001b[38;5;241m.\u001b[39mhyp_combs[n]\n\u001b[1;32m----> 4\u001b[0m trc \u001b[38;5;241m=\u001b[39m \u001b[43mvl_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtr_err_curves\u001b[49m[hyp_comb]\n\u001b[0;32m      5\u001b[0m vlc \u001b[38;5;241m=\u001b[39m vl_method\u001b[38;5;241m.\u001b[39mvl_err_curves[hyp_comb]\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(trc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KFoldCrossValidation' object has no attribute 'tr_err_curves'"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "\n",
    "hyp_comb = vl_method.hyp_combs[n]\n",
    "trc = vl_method.tr_err_curves[hyp_comb]\n",
    "vlc = vl_method.vl_err_curves[hyp_comb]\n",
    "\n",
    "plt.plot(trc)\n",
    "plt.plot(vlc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
