{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from __future__ import annotations\n",
    "import itertools\n",
    "%matplotlib widget\n",
    "\n",
    "from typing import TypeAlias, Iterator\n",
    "Vector: TypeAlias = np.ndarray  # A 1-D array\n",
    "Matrix: TypeAlias = np.ndarray  # A 2-D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerInitializationStrategy:\n",
    "    \"\"\"Parent class for initialization strategies of weights and biases in Layer.\"\"\"\n",
    "    def run(self, size_of_previous_layer: int, size_of_current_layer: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "        raise NotImplementedError(\"The 'run' method is implemented only in child classes\")\n",
    "\n",
    "class RandomUniform(LayerInitializationStrategy):\n",
    "    \"\"\"\n",
    "    Initialization strategy sampling weights and biases uniformly in a given interval.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    scale: float\n",
    "        The half-lenght of the interval [center-scale, center+scale] from which values are sampled.\n",
    "    center: float\n",
    "        The center of the aforementioned interval.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    run(self, size_of_previous_layer: int, size_of_current_layer: int) -> tuple[np.ndarray, np.ndarray]\n",
    "        Returns the tuple (random_weights, random_biases), where random_weights and random_biases are np.ndarrays of the appropriate shape.\n",
    "    \"\"\"\n",
    "    def __init__(self, scale_of_interval: float, center_of_interval: float = 0):\n",
    "        self.scale: float = scale_of_interval\n",
    "        self.center: float = center_of_interval\n",
    "        \n",
    "    \n",
    "    def run(self, size_of_previous_layer: int, size_of_current_layer: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "        random_weights: np.ndarray = np.random.uniform(\n",
    "            -self.scale + self.center, self.scale + self.center,\n",
    "            (size_of_previous_layer, size_of_current_layer)\n",
    "            )\n",
    "        random_biases: np.ndarray = np.random.uniform(\n",
    "            -self.scale + self.center, self.scale + self.center,\n",
    "            size_of_current_layer\n",
    "            )\n",
    "        return random_weights, random_biases\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Initialization: RandomUniform in [{self.center - self.scale}, {self.center + self.scale}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    \"\"\"Parent class for activation functions of neural nodes.\"\"\"\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError(\"The '__call__' method must be implemented in child classes\")\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError(\"The 'derivative' method must be implemented in child classes\")\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    \"\"\"Sigmoid activation function. f(x) = 1 / (1 + np.exp(-x))\"\"\"\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        sigmoid = self(x)  # Reuse the __call__ method to compute sigmoid\n",
    "        return sigmoid * (1 - sigmoid)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Sigmoid act. fun.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A Layer component of a NeuralNetwork.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    unit_number: int\n",
    "        The number of nodes/units in the Layer.\n",
    "    init_strat: LayerInitializationStrategy\n",
    "        The initialization strategy for the weights and biases of the Layer\n",
    "    activation_function: ActivationFunction\n",
    "        The function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, number_of_units: int,\n",
    "                 initialization_strategy: LayerInitializationStrategy,\n",
    "                 activation_function: ActivationFunction):\n",
    "        self.unit_number: int = number_of_units  # The number of nodes/units in the Layer.\n",
    "        self.init_strat: LayerInitializationStrategy = initialization_strategy\n",
    "        self.activation_function: ActivationFunction = activation_function\n",
    "\n",
    "        # The values computed by the units, based on the outputs of the previous layer. Stored for later backprop.\n",
    "        self.linear_output: Vector = None\n",
    "        self.output: Vector = None\n",
    "        \n",
    "        # The layer preceding the current one in the Neural Network. The NN should connect layers during initialization.\n",
    "        self.previous_layer: Layer = None\n",
    "        self.next_layer: Layer = None\n",
    "\n",
    "        # Weights and biases connecting the layer with the previous layer of the neural network.\n",
    "        self.weights: Matrix = None; self.biases: Vector = None\n",
    "\n",
    "        # A variable that needs to be computed from the delta of next layer in the\n",
    "        # Backprop TrainingAlgorithm\n",
    "        self.delta: Vector = None\n",
    "        \n",
    "\n",
    "    def initialize_weights(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the weights and biases of this Layer according to its init_strat.\n",
    "        \"\"\"\n",
    "        self.weights, self.biases = self.init_strat.run(self.previous_layer.unit_number, self.unit_number)\n",
    "\n",
    "    def compute_output(self):\n",
    "        \"\"\"\n",
    "        Computes the output of this layer as activation_function(np.dot(input, weights) + biases), where\n",
    "        the input is the output of the previous layer.\n",
    "        Stores the output as well as just the linear_output np.dot(input, weights) + biases, as it's useful in typical training algorithms.\n",
    "        \"\"\"\n",
    "        self.linear_output = np.dot(self.previous_layer.output, self.weights) + self.biases\n",
    "        self.output: Vector = self.activation_function(self.linear_output)\n",
    "        return self.output\n",
    "\n",
    "class InputLayer(Layer):\n",
    "    \"\"\"\n",
    "    The first Layer of a NeuralNetwork. It has no previous layer, and thus no weights and biases to connect it with.\n",
    "    Its activation function is the Identity.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    unit_number: int\n",
    "        The number of nodes/units in the Layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, number_of_units: int):\n",
    "        super().__init__(number_of_units, None, None)\n",
    "        # An input layer has no previous layer to connect to, so attributes referring to a previous layer are deleted.\n",
    "        del self.previous_layer, self.weights, self.biases, self.init_strat, self.activation_function\n",
    "    \n",
    "    def feed_input(self, value: Vector) -> None:\n",
    "        \"\"\"\n",
    "        Sets the input (which is also the output) of the InputLayer, and thus of the whole NeuralNetwork, to value.\n",
    "        \"\"\"\n",
    "        self.output: Vector = value\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        raise NotImplementedError(\"InputLayer does not require weight initialization.\")\n",
    "\n",
    "    def compute_output(self) -> Vector:\n",
    "        \"\"\"\n",
    "        Returns the output (which is also the input) of the InputLayer\n",
    "        \"\"\"\n",
    "        return self.output\n",
    "\n",
    "class HiddenLayer(Layer):\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Computes the delta of this layer from the delta of the next layer\n",
    "        as np.dot(self.next_layer.weights, self.next_layer.delta) * self.activation_function.derivative(self.linear_output)\n",
    "        \"\"\"\n",
    "        self.delta = np.dot(self.next_layer.weights, self.next_layer.delta) * self.activation_function.derivative(self.linear_output)\n",
    "\n",
    "class OutputLayer(Layer):\n",
    "    def __init__(self, number_of_units: int,\n",
    "                 initialization_strategy: LayerInitializationStrategy,\n",
    "                 activation_function: ActivationFunction):\n",
    "        super().__init__(number_of_units, initialization_strategy, activation_function)\n",
    "        del self.next_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListOfArrays:\n",
    "    \"\"\"\n",
    "    An utility class for defining element-wise operations on lists containing heteromorphic np.ndarrays.\n",
    "    Useful for conveniently manipulating network weights and biases in MLP-architecture NeuralNetworks.\n",
    "    \"\"\"\n",
    "    def __init__(self, arrays: list[np.ndarray]):\n",
    "        self.arrays: list[np.ndarray] = arrays\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ListOfArrays{(self.arrays)}\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.arrays[index]\n",
    "\n",
    "    def __setitem__(self, index, value):\n",
    "        self.arrays[index] = value\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if not isinstance(other, ListOfArrays): raise TypeError(\"Operand is not a ListOfArrays\")\n",
    "        return ListOfArrays([x + y for x, y in zip(self.arrays, other.arrays)])\n",
    "    \n",
    "    def __mul__(self, scalar: float):\n",
    "        return ListOfArrays([x * scalar for x in self.arrays])\n",
    "    \n",
    "    def __rmul__(self, scalar: float):\n",
    "        return self.__mul__(scalar)\n",
    "\n",
    "    def __truediv__(self, scalar: float):\n",
    "        return ListOfArrays([x / scalar for x in self.arrays])\n",
    "    \n",
    "    def __pow__(self, power: float):\n",
    "        return ListOfArrays([x**power for x in self.arrays])\n",
    "    \n",
    "    def sum(self) -> float:\n",
    "        return np.sum([np.sum(array) for array in self.arrays])\n",
    "    \n",
    "    def set_all_values_to(self, value: float) -> None:\n",
    "        for a in self.arrays:\n",
    "            a = value*np.ones(a.shape)\n",
    "\n",
    "\n",
    "class ListOfVectors(ListOfArrays):\n",
    "    \"\"\"\n",
    "    An utility class for convenient manipulation of lists of Vectors of different lengths.\n",
    "    Useful for network biases in MLP-architecture NeuralNetworks.\n",
    "    \"\"\"\n",
    "    def __init__(self, arrays: list[Vector]):\n",
    "        super().__init__(arrays)\n",
    "\n",
    "class ListOfMatrices(ListOfArrays):\n",
    "    \"\"\"\n",
    "    An utility class for convenient manipulation of lists of Matrices of different number of rows and cols.\n",
    "    Useful for network weights in MLP-architecture NeuralNetworks.\n",
    "    \"\"\"\n",
    "    pass\n",
    "    def __init__(self, arrays: list[Matrix]):\n",
    "        super().__init__(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineLearningModel:\n",
    "    pass\n",
    "\n",
    "class NeuralNetworkArchitecture:\n",
    "    \"\"\"\n",
    "    An utility class for storing information about the number of Layers and of units for each Layer of a NeuralNetwork, as well as other\n",
    "    useful data.\n",
    "\n",
    "    Used for an alternative constructor of NeuralNetwork when no fine-control for initialization is needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes_of_layers: list[int], activation_function: ActivationFunction, initialization_strategy: LayerInitializationStrategy):\n",
    "        self.sizes_of_layers: list[int] = sizes_of_layers\n",
    "        self.activation_function: ActivationFunction = activation_function\n",
    "        self.initialization_strategy: LayerInitializationStrategy = initialization_strategy\n",
    "    \n",
    "    def __str__(self):\n",
    "        network_shape: str = \"(\" + \", \".join(map(str, self.sizes_of_layers)) + \")\"\n",
    "        return f\"{network_shape}\"\n",
    "\n",
    "class NeuralNetwork(MachineLearningModel):\n",
    "    \"\"\"\n",
    "    A NeuralNetwork.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layers: list[Layer]\n",
    "        The list of Layer that make the NeuralNetwork.\n",
    "        Most NeuralNetwork methods work by invoking the appropriate Layer-level methods in the appropriate order.\n",
    "    input_layer: Layer\n",
    "        The first Layer of the NN\n",
    "    hidden_layers: list[Layer]\n",
    "        The list of non-first-nor-last Layers.\n",
    "    output_layer: Layer\n",
    "        The last Layer of the NN\n",
    "    layers_with_weights: list[Layer]\n",
    "        The list of all Layers, except the InputLayer.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: list[Layer]):\n",
    "        self.layers: list[Layer] = layers\n",
    "        # Maybe here I should ensure that layers are correctly typed (layers[0] should be an InputLayer, layers[-1] an OutputLayer, all other layers should be HiddenLayer).\n",
    "        self.input_layer: InputLayer = layers[0]; self.hidden_layers: list[HiddenLayer] = layers[1: -1]; self.output_layer: OutputLayer = layers[-1]\n",
    "        self.layers_with_weights: list[Layer] = self.layers[1: ]\n",
    "        self.connect_layers()\n",
    "        self.initialize_weights()\n",
    "\n",
    "    @classmethod\n",
    "    def FromArchitecture(cls, architecture: NeuralNetworkArchitecture):\n",
    "        sizes: list[int] = architecture.sizes_of_layers; act_fun = architecture.activation_function; init_strat = architecture.initialization_strategy\n",
    "        il: InputLayer = InputLayer(sizes[0])\n",
    "        hls: list[HiddenLayer] = [HiddenLayer(n, init_strat, act_fun) for n in sizes[1: -1]]\n",
    "        ol: OutputLayer = OutputLayer(sizes[-1], init_strat, act_fun)\n",
    "        layers: list[Layer] = [il] + hls + [ol]\n",
    "        return cls(layers)\n",
    "\n",
    "    def connect_layers(self) -> None:\n",
    "        for (i, layer) in enumerate(self.layers):\n",
    "            if not isinstance(layer, InputLayer): layer.previous_layer = self.layers[i - 1]\n",
    "            if not isinstance(layer, OutputLayer): layer.next_layer = self.layers[i + 1]\n",
    "\n",
    "    def initialize_weights(self) -> None:\n",
    "        for layer in self.layers_with_weights: layer.initialize_weights()\n",
    "    \n",
    "    def feed_input(self, value: np.ndarray) -> None:\n",
    "        self.input_layer.feed_input(value)\n",
    "\n",
    "    def activate_network(self) -> np.ndarray:\n",
    "        for i in range(len(self.layers)): self.layers[i].compute_output()\n",
    "        return self.output_layer.output\n",
    "    \n",
    "    def compute_output(self, value: np.ndarray) -> np.ndarray:\n",
    "        self.feed_input(value)\n",
    "        return self.activate_network()\n",
    "    \n",
    "    def backward(self) -> None:\n",
    "        for l in reversed(self.hidden_layers):\n",
    "            l.backward()\n",
    "    \n",
    "    def compute_multiple_outputs(self, x_data: pd.DataFrame | np.ndarray) -> np.ndarray[np.ndarray]:\n",
    "        if isinstance(x_data, pd.DataFrame): x_data = x_data.to_numpy()\n",
    "        outputs = np.array(\n",
    "            [\n",
    "                self.compute_output(x_data[i]) for i in range(len(x_data))\n",
    "            ]\n",
    "        )\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizationTerm:\n",
    "    def set_network(self, network: NeuralNetwork) -> None:\n",
    "        self.network = network\n",
    "\n",
    "    def __call__(self) -> float:\n",
    "        pass\n",
    "\n",
    "    def gradient(self) -> tuple[ListOfArrays, ListOfArrays]:\n",
    "        pass\n",
    "\n",
    "class NoRegularization(RegularizationTerm):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self) -> float:\n",
    "        return 0\n",
    "    \n",
    "    def gradient(self) -> tuple[ListOfArrays, ListOfArrays]:\n",
    "        layers = self.network.layers_with_weights\n",
    "        return ListOfArrays([np.zeros_like(l.weights) for l in layers]), ListOfArrays([np.zeros_like(l.biases) for l in layers])\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"No reg.\"\n",
    "\n",
    "class Tikhonov(RegularizationTerm):\n",
    "    \"\"\"\n",
    "    A regularization penalty term of the form constant*(sum of squares of weights and biases).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    penalty: float\n",
    "        The constant factor multiplying the sum of squares.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    penalty: float\n",
    "        The constant factor multiplying the sum of squares.\n",
    "    network: NeuralNetwork\n",
    "        The NeuralNetwork that weights and biases are read from.\n",
    "    \"\"\"\n",
    "    def __init__(self, penalty: float):\n",
    "        self.penalty: float = penalty\n",
    "        self.network: NeuralNetwork = None\n",
    "\n",
    "    def __call__(self) -> float:\n",
    "        layers: list[Layer] = self.network.layers_with_weights\n",
    "        weights_term = np.sum([np.sum(layer.weigths**2) for layer in layers])  # The sum of squares of all the weights in the NN.\n",
    "        biases_term = np.sum([np.sum(layer.biases**2) for layer in layers])\n",
    "        return self.penalty * (weights_term + biases_term) / 2\n",
    "\n",
    "    def gradient(self) -> tuple[ListOfArrays, ListOfArrays]:\n",
    "        layers: list[Layer] = self.network.layers_with_weights\n",
    "        gradient_on_weights: ListOfArrays = ListOfArrays([-self.penalty * l.weights for l in layers])\n",
    "        gradient_on_biases: ListOfArrays = ListOfArrays([-self.penalty * l.biases for l in layers])\n",
    "        return gradient_on_weights, gradient_on_biases\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Tikhonov({self.penalty})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumRule:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingCondition:\n",
    "    def __init__(self):\n",
    "        self.alg: TrainingAlgorithm = None\n",
    "    \n",
    "    def set_alg(self, alg: TrainingAlgorithm) -> None:\n",
    "        self.alg = alg\n",
    "\n",
    "    @property\n",
    "    def is_satisfied(self) -> bool:\n",
    "        pass\n",
    "\n",
    "class ThresholdOnTrainingError(StoppingCondition):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    threshold: float\n",
    "    patience: int\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold: float, patience: int):\n",
    "        super().__init__()\n",
    "        self.threshold: float = threshold\n",
    "        self.patience: int = patience\n",
    "    \n",
    "    @property\n",
    "    def is_satisfied(self) -> bool:\n",
    "        current_training_error: float = self.alg.current_tr_err\n",
    "        if current_training_error < self.threshold:\n",
    "            self.consecutive_epochs += 1\n",
    "            return self.consecutive_epochs > self.patience\n",
    "        else:\n",
    "            self.consecutive_epochs = 0\n",
    "            return False\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"TR Err threshold: {self.threshold}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorFunction:\n",
    "    def __call__(self, y_data: np.ndarray, y_predicted: np.ndarray) -> float:\n",
    "        pass\n",
    "    \n",
    "    def simple_gradient(self, y_data: np.ndarray, y_predicted: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "class MSE(ErrorFunction):\n",
    "    def __call__(self, y_data: np.ndarray, y_predicted: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Returns the average over the dataset of the square euclidean distance between the training outputs and the predictions.\n",
    "        \"\"\"\n",
    "        num_patterns = 1 if y_data.ndim == 1 else len(y_data)\n",
    "        return 0.5 * np.sum((y_data - y_predicted)**2) / num_patterns\n",
    "    \n",
    "    def simple_gradient(self, y_data: np.ndarray, y_predicted: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns y_data - y_predicted. It is meant to be used on a single pattern at a time, during backpropagation.\n",
    "        \"\"\"\n",
    "        return (y_data - y_predicted)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"MSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, x_data: Matrix, y_data: Matrix):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with input (x_data) and output (y_data).\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(x_data, pd.DataFrame): x_data = x_data.to_numpy()\n",
    "        if isinstance(y_data, pd.DataFrame): y_data = y_data.to_numpy()\n",
    "        \n",
    "        if x_data.ndim == 1 or y_data.ndim == 1:\n",
    "            raise ValueError(f\"x_data and y_data should be matrices, where each row represents a pattern and each column a feature, but got arguments of shape {x_data.shape} and {y_data.shape}\")\n",
    "\n",
    "        self.x: Matrix = x_data\n",
    "        self.y: Matrix = y_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of patterns in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves the input-output pair at the specified index.\n",
    "        \"\"\"\n",
    "        if isinstance(index, slice):\n",
    "            return Dataset(self.x[index], self.y[index])\n",
    "        # This should be refactored in the future. The behaviour should be the same as numpy, regardless of index type.\n",
    "        elif isinstance(index, list) or isinstance(index, np.ndarray):\n",
    "            return Dataset(self.x[index], self.y[index])\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def shuffle(self) -> None:\n",
    "        indices: np.ndarray = np.arange(len(self))\n",
    "        np.random.shuffle(indices)\n",
    "        self.x = self.x[indices, :]\n",
    "        self.y = self.y[indices, :]\n",
    "    \n",
    "    def split(self, fraction: float, shuffle: bool = True) -> tuple[Dataset, Dataset]:\n",
    "        \"\"\"\n",
    "        Returns two datasets, one with fraction*N data and the other with (1-fraction)*N.\n",
    "        \"\"\"\n",
    "        indices: np.ndarray = np.arange(len(self))\n",
    "        \n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        splitting_number: int = int(fraction * len(self))\n",
    "        indices_1: np.ndarray = indices[:splitting_number]\n",
    "        \n",
    "        indices_2: np.ndarray = indices[splitting_number:]\n",
    "        ds_1: Dataset = Dataset(self.x[indices_1, :], self.y[indices_1, :])\n",
    "        ds_2: Dataset = Dataset(self.x[indices_2, :], self.y[indices_2, :])\n",
    "        return ds_1, ds_2\n",
    "\n",
    "\n",
    "class DataManager:\n",
    "    \"\"\"An auxiliary class for extracting minibatches from a Dataset.\"\"\"\n",
    "    def __init__(self, dataset: Dataset, minibatch_size: int = None, shuffle: bool = True):\n",
    "        self.dataset: Dataset = dataset\n",
    "        self.minibatch_size: int = minibatch_size or len(dataset)\n",
    "        self.shuffle: bool = shuffle\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        An iterator yielding minibatches.\n",
    "        \"\"\"\n",
    "        indices = np.arange(len(self.dataset))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        for start in range(0, len(indices), self.minibatch_size):\n",
    "            minibatch_indices = indices[start:start + self.minibatch_size]\n",
    "            minibatch_x = self.dataset.x[minibatch_indices, :]\n",
    "            minibatch_y = self.dataset.y[minibatch_indices, :]\n",
    "            yield minibatch_x, minibatch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingAlgorithm:\n",
    "    def __init__(self, x_train: pd.DataFrame, y_train: pd.DataFrame, network: NeuralNetwork):\n",
    "        self.network: NeuralNetwork = network\n",
    "\n",
    "        if isinstance(x_train, pd.DataFrame): x_train = x_train.to_numpy()\n",
    "        if isinstance(y_train, pd.DataFrame): y_train = y_train.to_numpy()\n",
    "\n",
    "        self.training_set = Dataset(x_train, y_train)\n",
    "\n",
    "        self.current_tr_err: float = float('inf')\n",
    "\n",
    "        self.history: dict[list] = {'training error': []}\n",
    "        self.cache: dict = {}\n",
    "\n",
    "class Backprop(TrainingAlgorithm):\n",
    "    def __init__(self, x_train: pd.DataFrame, y_train: pd.DataFrame, network: NeuralNetwork,\n",
    "                 learning_rate: float,\n",
    "                 error_function: ErrorFunction,\n",
    "                 stopping_condition: StoppingCondition,\n",
    "                 regularization_term: RegularizationTerm = None,\n",
    "                 minibatch_size: int = None\n",
    "                 ):\n",
    "        super().__init__(x_train, y_train, network)\n",
    "\n",
    "        self.learning_rate: float = learning_rate\n",
    "\n",
    "        self.err_fun: ErrorFunction = error_function\n",
    "        \n",
    "        self.stop_cond: StoppingCondition = stopping_condition\n",
    "        self.stop_cond.set_alg(self)\n",
    "\n",
    "        self.weights_gradient: ListOfMatrices = ListOfMatrices([np.zeros_like(l.weights) for l in self.network.layers_with_weights])\n",
    "        self.biases_gradient: ListOfVectors = ListOfVectors([np.zeros_like(l.biases) for l in self.network.layers_with_weights])\n",
    "\n",
    "        self.regularization_term: RegularizationTerm = regularization_term or NoRegularization()\n",
    "        self.regularization_term.set_network(self.network)\n",
    "\n",
    "        self.minibatch_size: int = minibatch_size or len(self.training_set)\n",
    "        self.minibatch_generator: DataManager = DataManager(self.training_set, self.minibatch_size,\n",
    "                                                            shuffle = (self.minibatch_size != len(self.training_set))\n",
    "                                                            )\n",
    "        \n",
    "        \n",
    "        self.current_mb_size: int = None\n",
    "        self.current_mb_x: Matrix = None; self.current_mb_y: Matrix = None\n",
    "        \n",
    "    \n",
    "    def run(self, max_epochs: int) -> None:\n",
    "        epoch: int = 0\n",
    "        while epoch < max_epochs:\n",
    "            epoch += 1\n",
    "\n",
    "            for minibatch_x, minibatch_y in self.minibatch_generator:\n",
    "                self.update_minibatch_metadata(minibatch_x, minibatch_y)\n",
    "                self.update_gradients()\n",
    "                self.update_network_parameters()\n",
    "            \n",
    "            self.compute_training_error()\n",
    "            if self.stop_cond.is_satisfied: break\n",
    "\n",
    "    def update_minibatch_metadata(self, minibatch_x: Matrix, minibatch_y: Matrix):\n",
    "        self.current_mb_x = minibatch_x; self.current_mb_y = minibatch_y\n",
    "        self.current_mb_size = len(minibatch_x)\n",
    "    \n",
    "    def update_gradients(self):\n",
    "        self.reset_gradients()\n",
    "\n",
    "        for x, y in zip(self.current_mb_x, self.current_mb_y):\n",
    "            predicted_y = self.network.compute_output(x)\n",
    "            out_l = self.network.output_layer\n",
    "            out_l.delta = self.err_fun.simple_gradient(y, predicted_y)*out_l.activation_function.derivative(out_l.linear_output)\n",
    "            self.network.backward()\n",
    "            self.weights_gradient += ListOfMatrices([np.outer(l.previous_layer.output, l.delta) for l in self.network.layers_with_weights])\n",
    "            self.biases_gradient += ListOfVectors([l.delta for l in self.network.layers_with_weights])\n",
    "        self.weights_gradient /= self.current_mb_size; self.biases_gradient /= self.current_mb_size\n",
    "\n",
    "        self.add_regul_contribution()\n",
    "        self.add_momentum_contribution()\n",
    "    \n",
    "    def reset_gradients(self) -> None:\n",
    "        self.weights_gradient.set_all_values_to(0)\n",
    "        self.biases_gradient.set_all_values_to(0)\n",
    "    \n",
    "    def add_regul_contribution(self) -> None:\n",
    "        contribution_to_w, contribution_to_b = self.regularization_term.gradient()\n",
    "        self.weights_gradient += contribution_to_w; self.biases_gradient += contribution_to_b\n",
    "\n",
    "    def add_momentum_contribution(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def update_network_parameters(self) -> None:\n",
    "        factor = self.learning_rate * self.current_mb_size / len(self.training_set)\n",
    "        \n",
    "        for i, l in enumerate(self.network.layers_with_weights):\n",
    "            l.weights += factor * self.weights_gradient[i]\n",
    "            l.biases += factor * self.biases_gradient[i]\n",
    "\n",
    "    def compute_training_error(self) -> None:\n",
    "        y_prediction = self.network.compute_multiple_outputs(self.training_set.x)\n",
    "        self.current_tr_err = self.err_fun(self.training_set.y, y_prediction)\n",
    "        self.history['training error'] += [self.current_tr_err]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterGrid:\n",
    "    def __init__(self, algorithm_class: type[TrainingAlgorithm], values_of_training_hyperparameters: dict[str, list[object]], list_of_architectures: list[NeuralNetworkArchitecture]):\n",
    "        self.algorithm_class: type[TrainingAlgorithm] = algorithm_class\n",
    "        self.tr_hyparams: dict[str, list] = values_of_training_hyperparameters\n",
    "        self.check_hyperparameter_properness()  # Check the given hyperparameter types are correct and complete for the algorithm class.\n",
    "        self.architectures: list[NeuralNetworkArchitecture] = list_of_architectures\n",
    "\n",
    "    def __iter__(self) -> Iterator[tuple[NeuralNetworkArchitecture, dict[str, object]]]:\n",
    "        for architecture in self.architectures:\n",
    "            for hyperparameter_combination in itertools.product(*self.tr_hyparams.values()):\n",
    "                tr_hyparam_comb_dict: dict[str, object] = dict(zip(self.tr_hyparams.keys(), hyperparameter_combination))\n",
    "                yield architecture, tr_hyparam_comb_dict\n",
    "\n",
    "    def to_list(self) -> list[tuple[NeuralNetworkArchitecture, dict[str, object]]]:\n",
    "        return [x for x in self]\n",
    "    \n",
    "    def check_hyperparameter_properness(self) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluationMethod:\n",
    "    pass\n",
    "\n",
    "class SelectionMethod(ModelEvaluationMethod):\n",
    "    def __init__(self, dataset: Dataset, algorithm_class: type[TrainingAlgorithm], hyperparameter_combinations: list[tuple[NeuralNetworkArchitecture, dict]]):\n",
    "        self.dataset: Dataset = dataset\n",
    "        self.algorithm_class: type[TrainingAlgorithm] = algorithm_class\n",
    "        self.hyperparameter_combinations: list[tuple[NeuralNetworkArchitecture, dict]] = hyperparameter_combinations\n",
    "\n",
    "        self.check_hyperparameter_properness()\n",
    "    \n",
    "    @classmethod\n",
    "    def FromGrid(cls, dataset: Dataset, grid: HyperparameterGrid):\n",
    "        alg_class: type[TrainingAlgorithm] = grid.algorithm_class\n",
    "        list_of_combinations: list[tuple[NeuralNetworkArchitecture, dict]] = grid.to_list()\n",
    "        return cls(dataset, alg_class, list_of_combinations)\n",
    "\n",
    "    def check_hyperparameter_properness(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def hyperparam_keys(self) -> list[str]:\n",
    "        architecture_keys = [\"Architecture\"]\n",
    "        training_hyperparameters_keys = [list(item[1].keys()) for item in self.hyperparameter_combinations]\n",
    "        training_hyperparameters_keys = list(set().union(*[x for x in training_hyperparameters_keys]))\n",
    "        return architecture_keys + training_hyperparameters_keys\n",
    "\n",
    "class AssessmentMethod(ModelEvaluationMethod):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parenthetical_uncertainty_format(number: float, uncertainty: float, uncertainty_digits: int = 2) -> str:\n",
    "    import math\n",
    "    num_exponent = math.floor(math.log10(abs(number))) if number != 0 else 0\n",
    "    unc_exponent = math.floor(math.log10(abs(uncertainty))) if uncertainty != 0 else 0\n",
    "\n",
    "    n = num_exponent - unc_exponent + 1  # Number of significant digits\n",
    "    if n <= 0: return f\"{number} +- {uncertainty}\"\n",
    "    scaled_number = number * 10 ** -num_exponent\n",
    "    num_string = f\"{scaled_number:.{n - 1 + uncertainty_digits - 1}f}\"\n",
    "\n",
    "    scaled_uncertainty = uncertainty * 10 ** -unc_exponent\n",
    "    unc_string = f\"{scaled_uncertainty:.{uncertainty_digits - 1}f}\".replace('.', '')\n",
    "\n",
    "    return f\"{num_string}({unc_string})e{num_exponent}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoldOutSelection(SelectionMethod):\n",
    "    def __init__(self, dataset: Dataset | tuple[Dataset, Dataset], validation_split: float,\n",
    "                 algorithm_class: type[TrainingAlgorithm], hyperparameter_combinations: list[tuple[NeuralNetworkArchitecture, dict]],\n",
    "                 risk_function: ErrorFunction,\n",
    "                 shuffle_data: bool = True):\n",
    "        super().__init__(dataset, algorithm_class, hyperparameter_combinations)\n",
    "\n",
    "        if isinstance(dataset, Dataset):\n",
    "            self.vl_set, self.tr_set = self.dataset.split(validation_split, shuffle_data)\n",
    "        elif isinstance(dataset, tuple):\n",
    "            self.tr_set, self.vl_set = dataset\n",
    "\n",
    "        \n",
    "        self.risk_fun: ErrorFunction = risk_function\n",
    "        \n",
    "        self.results: list[dict] = []\n",
    "    \n",
    "    def run(self, max_epochs: int, trace_validation_error: bool = True) -> None:\n",
    "        for architecture, training_hyperparameters in self.hyperparameter_combinations:\n",
    "            network: NeuralNetwork = NeuralNetwork.FromArchitecture(architecture)\n",
    "            alg: TrainingAlgorithm = self.algorithm_class(self.tr_set.x, self.tr_set.y, network, **training_hyperparameters)\n",
    "\n",
    "            if trace_validation_error:\n",
    "                vl_err_history: list[float] = []\n",
    "                for _ in range(max_epochs):\n",
    "                    alg.run(1)\n",
    "                    y_predicted: np.ndarray = network.compute_multiple_outputs(self.vl_set.x)\n",
    "                    vl_error: float = self.risk_fun(self.vl_set.y, y_predicted)\n",
    "                    vl_err_history += [vl_error]\n",
    "            else:\n",
    "                alg.run(max_epochs)\n",
    "                # Measure and store the validation error\n",
    "                y_predicted: np.ndarray = network.compute_multiple_outputs(self.vl_set.x)\n",
    "                vl_error: float = self.risk_fun(self.vl_set.y, y_predicted)\n",
    "\n",
    "            \n",
    "\n",
    "            result: dict = {\n",
    "                \"Architecture\": str(architecture),\n",
    "                **{k: str(v) if isinstance(v, (int, float)) else v for k, v in training_hyperparameters.items()},\n",
    "                \"Validation Error\": vl_error\n",
    "            }\n",
    "\n",
    "            if trace_validation_error: result.update({\"TR Curve\": alg.history[\"training error\"], \"VL Curve\": vl_err_history})\n",
    "\n",
    "            self.results.append(result)\n",
    "    \n",
    "    def dataframe(self) -> pd.DataFrame:\n",
    "        tr_hyperparams_keys = self.hyperparameter_combinations[0][1].keys()\n",
    "        columns: list[str] = ['Architecture', *tr_hyperparams_keys, \"Validation Error\"]\n",
    "        return pd.DataFrame(self.results, columns= columns)\n",
    "\n",
    "class KFoldCrossValidation(SelectionMethod):\n",
    "    def __init__(self, dataset: Dataset, number_of_folds: int, algorithm_class: type[TrainingAlgorithm], hyperparameter_combinations,\n",
    "                 risk_function: ErrorFunction,\n",
    "                 shuffle_data: bool = True):\n",
    "        super().__init__(dataset, algorithm_class, hyperparameter_combinations)\n",
    "        self.n_folds: int = number_of_folds\n",
    "        if shuffle_data: self.dataset.shuffle()\n",
    "        fold_length: int = int(len(self.dataset) / number_of_folds)\n",
    "        self.folds: list[Dataset] = [self.dataset[start:start + fold_length] for start in range(number_of_folds)]\n",
    "        self.risk_fun = risk_function\n",
    "        \n",
    "        \n",
    "        self.results: list[dict] = []\n",
    "        self.aggregated_results: list[dict] = []\n",
    "\n",
    "    \n",
    "    def run(self, max_epochs: int) -> None:\n",
    "        for k in range(self.n_folds):\n",
    "            vl_set: Dataset = self.folds[k]\n",
    "            tr_set: Dataset = Dataset(\n",
    "                np.concatenate([self.folds[j].x for j in range(self.n_folds) if j != k]),\n",
    "                np.concatenate([self.folds[j].y for j in range(self.n_folds) if j != k])\n",
    "            )\n",
    "\n",
    "            ## Perform hold out validation using vl_set as validation set and tr_set as training set,\n",
    "            ## and store the performance for each combination of hyperparameters to extract mean and stdev later.\n",
    "            hold_out = HoldOutSelection(\n",
    "                dataset = (tr_set, vl_set),\n",
    "                validation_split = None,\n",
    "                algorithm_class = self.algorithm_class,\n",
    "                hyperparameter_combinations = self.hyperparameter_combinations,\n",
    "                risk_function = self.risk_fun,\n",
    "                shuffle_data = False\n",
    "            )\n",
    "\n",
    "            hold_out.run(max_epochs, trace_validation_error = False)\n",
    "\n",
    "            fold_results: list[dict] = hold_out.results\n",
    "            self.results += fold_results\n",
    "    \n",
    "    def aggregate_result(self) -> None:\n",
    "\n",
    "        self.aggregated_results = []\n",
    "\n",
    "        auxiliary_dictionary: dict = {}\n",
    "        for item in self.results:\n",
    "            composite_key: tuple = tuple((k, str(item[k])) for k in self.hyperparam_keys)\n",
    "            value = item[\"Validation Error\"]\n",
    "            print(composite_key)\n",
    "            if composite_key not in auxiliary_dictionary:\n",
    "                auxiliary_dictionary[composite_key] = []\n",
    "            auxiliary_dictionary[composite_key].append(value)\n",
    "    \n",
    "        for composite_key, list_of_vl_errors in auxiliary_dictionary.items():\n",
    "            result_dic = dict(composite_key)\n",
    "            result_dic[\"Validation Errors\"] = np.array(list_of_vl_errors)\n",
    "            self.aggregated_results.append(result_dic)\n",
    "        \n",
    "        for item in self.aggregated_results:\n",
    "            item[\"Validation Error\"] = f\"{np.mean(item[\"Validation Errors\"])} +- {np.std(item[\"Validation Errors\"], ddof = 1)}\"\n",
    "            item[\"Validation Error\"] = parenthetical_uncertainty_format(np.mean(item[\"Validation Errors\"]), np.std(item[\"Validation Errors\"], ddof = 1))\n",
    "\n",
    "    def dataframe(self) -> pd.DataFrame:\n",
    "        tr_hyperparams_keys = self.hyperparameter_combinations[0][1].keys()\n",
    "        columns: list[str] = ['Architecture', *tr_hyperparams_keys, \"Validation Error\"]\n",
    "        return pd.DataFrame(self.aggregated_results, columns= columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is subset a view? True\n",
      "[[0]\n",
      " [1]]\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 4]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset(np.array([[0], [1], [2]]), np.array([[3], [4], [5]]))\n",
    "subset = ds[0:2]\n",
    "\n",
    "print(\"Is subset a view?\", subset.x.base is ds.x)\n",
    "\n",
    "print(subset.x)\n",
    "\n",
    "\n",
    "a = np.array([[0], [1], [2]]); b = np.array([[3], [4], [5]])\n",
    "mask = np.ones((len(a), 1), dtype=bool)\n",
    "\n",
    "c = np.concatenate((a, b))\n",
    "\n",
    "print(c)\n",
    "[n for n in range(5) if n != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'Tikhonov(0.05)'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'Tikhonov(5e-05)'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'No reg.'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'Tikhonov(0.05)'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'Tikhonov(5e-05)'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'No reg.'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'Tikhonov(0.05)'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'Tikhonov(5e-05)'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'No reg.'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'Tikhonov(0.05)'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'Tikhonov(5e-05)'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'No reg.'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'Tikhonov(0.05)'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'Tikhonov(5e-05)'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'No reg.'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'Tikhonov(0.05)'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'Tikhonov(5e-05)'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n",
      "(('Architecture', '(4, 7, 3)'), ('error_function', 'MSE'), ('regularization_term', 'No reg.'), ('minibatch_size', 'None'), ('stopping_condition', 'TR Err threshold: 0.0001'), ('learning_rate', '0.9'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>error_function</th>\n",
       "      <th>stopping_condition</th>\n",
       "      <th>regularization_term</th>\n",
       "      <th>minibatch_size</th>\n",
       "      <th>Validation Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(4, 7, 3)</td>\n",
       "      <td>0.9</td>\n",
       "      <td>MSE</td>\n",
       "      <td>TR Err threshold: 0.0001</td>\n",
       "      <td>Tikhonov(0.05)</td>\n",
       "      <td>None</td>\n",
       "      <td>3.255(70)e-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(4, 7, 3)</td>\n",
       "      <td>0.9</td>\n",
       "      <td>MSE</td>\n",
       "      <td>TR Err threshold: 0.0001</td>\n",
       "      <td>Tikhonov(5e-05)</td>\n",
       "      <td>None</td>\n",
       "      <td>2.01(36)e-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(4, 7, 3)</td>\n",
       "      <td>0.9</td>\n",
       "      <td>MSE</td>\n",
       "      <td>TR Err threshold: 0.0001</td>\n",
       "      <td>No reg.</td>\n",
       "      <td>None</td>\n",
       "      <td>1.91(27)e-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Architecture learning_rate error_function        stopping_condition  \\\n",
       "0    (4, 7, 3)           0.9            MSE  TR Err threshold: 0.0001   \n",
       "1    (4, 7, 3)           0.9            MSE  TR Err threshold: 0.0001   \n",
       "2    (4, 7, 3)           0.9            MSE  TR Err threshold: 0.0001   \n",
       "\n",
       "  regularization_term minibatch_size Validation Error  \n",
       "0      Tikhonov(0.05)           None     3.255(70)e-1  \n",
       "1     Tikhonov(5e-05)           None      2.01(36)e-2  \n",
       "2             No reg.           None      1.91(27)e-2  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "iris_path = os.path.join('..', 'iris', 'iris.data')\n",
    "iris_df = pd.read_csv(iris_path, names = ['sepal length', 'sepal width', 'petal length', 'petal width', 'class'])\n",
    "iris_x = iris_df.loc[:, 'sepal length':'petal width']; iris_y = pd.get_dummies(iris_df.loc[:, 'class']).astype(int)\n",
    "\n",
    "hyperparams: dict[list] = {\n",
    "    'learning_rate': [0.9],\n",
    "    'error_function': [MSE()],\n",
    "    'stopping_condition': [ThresholdOnTrainingError(0.0001, 10)],\n",
    "    'regularization_term': [Tikhonov(5e-2),\n",
    "                            Tikhonov(5e-5),\n",
    "                            NoRegularization()\n",
    "                            ],\n",
    "    'minibatch_size': [None]\n",
    "}\n",
    "\n",
    "grid = HyperparameterGrid(\n",
    "    Backprop,\n",
    "    hyperparams,\n",
    "    list_of_architectures = [\n",
    "        NeuralNetworkArchitecture([4, 7, 3], Sigmoid(), RandomUniform(0.3))\n",
    "    ]\n",
    ")\n",
    "\n",
    "validation_method = KFoldCrossValidation(\n",
    "    Dataset(iris_x, iris_y),\n",
    "    number_of_folds=6,\n",
    "    algorithm_class=Backprop,\n",
    "    hyperparameter_combinations= grid.to_list(),\n",
    "    risk_function=MSE(),\n",
    "    shuffle_data=True\n",
    ")\n",
    "\n",
    "validation_method.run(1000)\n",
    "validation_method.aggregate_result()\n",
    "validation_method.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parenthetical_uncertainty_format(number: float, uncertainty: float, uncertainty_digits: int = 2) -> str:\n",
    "    import math\n",
    "    num_exponent = math.floor(math.log10(abs(number))) if number != 0 else 0\n",
    "    unc_exponent = math.floor(math.log10(abs(uncertainty))) if uncertainty != 0 else 0\n",
    "\n",
    "    n = num_exponent - unc_exponent + 1  # Number of significant digits\n",
    "    if n <= 0: return f\"{number} +- {uncertainty}\"\n",
    "    scaled_number = number * 10 ** -num_exponent\n",
    "    num_string = f\"{scaled_number:.{n - 1 + uncertainty_digits - 1}f}\"\n",
    "\n",
    "    scaled_uncertainty = uncertainty * 10 ** -unc_exponent\n",
    "    unc_string = f\"{scaled_uncertainty:.{uncertainty_digits - 1}f}\".replace('.', '')\n",
    "\n",
    "    return f\"{num_string}({unc_string})e{num_exponent}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8172(1384)e-1\n"
     ]
    }
   ],
   "source": [
    "print(parenthetical_uncertainty_format(0.1817191322531631, 0.01383920501477125, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parenthetical_uncertainty_format(number: float, uncertainty: float, uncertainty_digits: int = 2) -> str:\n",
    "    import math\n",
    "    exponent = math.floor(math.log10(abs(uncertainty))) if uncertainty != 0 else 0\n",
    "    scale = 10 ** -exponent\n",
    "\n",
    "    scaled_numer = number  * scale; scaled_unc = uncertainty * scale\n",
    "    rounded_unc = round(scaled_unc, uncertainty_digits - 1)\n",
    "    uncertainty_string = f\"{rounded_unc:.{uncertainty_digits - 1}f}\".replace('.', '')\n",
    "\n",
    "    number_string = f\"{scaled_numer:.{uncertainty_digits - 1}f}\".rstrip('0').rstrip('.')\n",
    "    return f\"{number_string}({uncertainty_string})e{exponent}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181.7(24)e-3\n"
     ]
    }
   ],
   "source": [
    "print(parenthetical_uncertainty_format(0.1817191322531631, 0.0024035738106961565))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(('Architecture', '(4, 7, 3)'), ('minibatch_size', 'None'), ('learning_rate', '0.9'), ('error_function', 'MSE'), ('regularization_term', 'Tikhonov(5e-05)'), ('stopping_condition', 'TR Err threshold: 0.0001')) == (('Architecture', '(4, 7, 3)'), ('minibatch_size', 'None'), ('learning_rate', '0.9'), ('error_function', 'MSE'), ('regularization_term', 'Tikhonov(5e-05)'), ('stopping_condition', 'TR Err threshold: 0.0001'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(validation_method.results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "iris_path = os.path.join('..', 'iris', 'iris.data')\n",
    "iris_df = pd.read_csv(iris_path, names = ['sepal length', 'sepal width', 'petal length', 'petal width', 'class'])\n",
    "iris_x = iris_df.loc[:, 'sepal length':'petal width']; iris_y = pd.get_dummies(iris_df.loc[:, 'class']).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hyperparams: dict[list] = {\n",
    "    'learning_rate': [0.9],\n",
    "    'error_function': [MSE()],\n",
    "    'stopping_condition': [ThresholdOnTrainingError(0.0001, 10)],\n",
    "    'regularization_term': [Tikhonov(5e-3),\n",
    "                            Tikhonov(5e-5),\n",
    "                            NoRegularization()\n",
    "                            ],\n",
    "    'minibatch_size': [None]\n",
    "}\n",
    "\n",
    "grid = HyperparameterGrid(\n",
    "    Backprop,\n",
    "    hyperparams,\n",
    "    list_of_architectures = [\n",
    "        NeuralNetworkArchitecture([4, 7, 3], Sigmoid(), RandomUniform(0.3))\n",
    "    ]\n",
    ")\n",
    "\n",
    "validation_method = HoldOutSelection(\n",
    "    Dataset(iris_x, iris_y),\n",
    "    0.85,\n",
    "    Backprop,\n",
    "    grid.to_list(),\n",
    "    MSE(),\n",
    "    shuffle_data = True\n",
    ")\n",
    "\n",
    "\n",
    "validation_method2 = HoldOutSelection(\n",
    "    Dataset(iris_x, iris_y),\n",
    "    0.85,\n",
    "    Backprop,\n",
    "    grid.to_list(),\n",
    "    MSE(),\n",
    "    shuffle_data = True\n",
    ")\n",
    "\n",
    "validation_method.run(500, trace_validation_error = True)\n",
    "validation_method2.run(500, trace_validation_error = False)\n",
    "df = validation_method.dataframe()\n",
    "df2 = validation_method2.dataframe()\n",
    "display(df, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict1 = validation_method.results\n",
    "list_dict2 = validation_method2.results\n",
    "\n",
    "total_list_of_dicts = list_dict1 + list_dict2\n",
    "# forbidden_keys = ['Validation Error', 'VL Curve', 'TR Curve']\n",
    "\n",
    "aggregate = {}\n",
    "\n",
    "for dic in total_list_of_dicts:\n",
    "    composite_key = tuple( (k, dic[k])   for k in dic if k in validation_method.hyperparam_keys )\n",
    "    value = dic[\"Validation Error\"]\n",
    "    if composite_key not in aggregate:\n",
    "        aggregate[composite_key] = []\n",
    "    aggregate[composite_key] += [value]\n",
    "\n",
    "final_list = []\n",
    "\n",
    "for composite_key, list_of_vl_errors in aggregate.items():\n",
    "    result_dict = dict(composite_key)\n",
    "    result_dict[\"Validation Errors\"] = np.array(list_of_vl_errors)\n",
    "    final_list.append(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identification_keys = [\"Architecture\", *[k for k in [validation_method.hyperparameter_combinations[j][1].keys() for j in len(validation_method.hyperparameter_combinations)]]]\n",
    "identification_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "**{k: str(v) if isinstance(v, (int, float)) else v for k, v in training_hyperparameters.items()}\n",
    "for arch, trhyperp in grid.to_list():\n",
    "    print(arch)\n",
    "    print(trhyperp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:v }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [0, 1, 2]\n",
    "for n in ns:\n",
    "    tr_hist, vl_hist = validation_method.results[n][\"TR Curve\"], validation_method.results[n][\"VL Curve\"]\n",
    "\n",
    "    plt.plot(tr_hist, ls = '--')\n",
    "    plt.plot(vl_hist)\n",
    "#plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "monk2train_path = os.path.join('..', 'monk+s+problems', 'monks-2.train')\n",
    "df = pd.read_csv(monk2train_path, sep = ' ', skipinitialspace= True, names= ['class', 'a1','a2','a3','a4','a5','a6', 'ID'])\n",
    "raw_x = df.loc[:, 'a1':'a6']; raw_y = df.loc[:, 'class']\n",
    "x = pd.get_dummies(raw_x, columns = raw_x.columns).astype(int); y = pd.get_dummies(raw_y).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hyperparams: dict[list] = {\n",
    "    'learning_rate': [200],\n",
    "    'error_function': [MSE()],\n",
    "    'stopping_condition': [ThresholdOnTrainingError(0.0001, 10)],\n",
    "    'regularization_term': [#Tikhonov(5e-12),\n",
    "                            #Tikhonov(5e-11),\n",
    "                            Tikhonov(1e-5)\n",
    "                            ],\n",
    "    'minibatch_size': [5]\n",
    "}\n",
    "\n",
    "grid = HyperparameterGrid(\n",
    "    Backprop,\n",
    "    hyperparams,\n",
    "    list_of_architectures = [\n",
    "        NeuralNetworkArchitecture([17, 3, 2], Sigmoid(), RandomUniform(0.3)),\n",
    "        #NeuralNetworkArchitecture([17, 4, 2], Sigmoid(), RandomUniform(0.3))\n",
    "    ]\n",
    ")\n",
    "\n",
    "validation_method = HoldOutSelection(\n",
    "    Dataset(x, y),\n",
    "    0.25,\n",
    "    Backprop,\n",
    "    grid.to_list(),\n",
    "    MSE(),\n",
    "    shuffle_data = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_method.run(500, trace_validation_error = True)\n",
    "df = validation_method.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [0]\n",
    "for n in ns:\n",
    "    tr_hist, vl_hist = validation_method.results[n][\"TR Curve\"], validation_method.results[n][\"VL Curve\"]\n",
    "\n",
    "    plt.plot(tr_hist, ls = '--')\n",
    "    plt.plot(vl_hist)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = InputLayer(17)\n",
    "hl = HiddenLayer(6, RandomUniform(0.2), Sigmoid())\n",
    "ol = OutputLayer(2, RandomUniform(0.2), Sigmoid())\n",
    "\n",
    "nn = NeuralNetwork([il, hl, ol])\n",
    "\n",
    "\n",
    "training_alg = Backprop(x.iloc[0:122], y.iloc[0:122], nn, 20.00, MSE(), ThresholdOnTrainingError(0.001, 10), regularization_term = Tikhonov(0.002), minibatch_size = 2)\n",
    "\n",
    "training_alg.run(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "monk2test_path = os.path.join('..', 'monk+s+problems', 'monks-2.test')\n",
    "df = pd.read_csv(monk2test_path, sep = ' ', skipinitialspace= True, names= ['class', 'a1','a2','a3','a4','a5','a6', 'ID'])\n",
    "raw_x = df.loc[:, 'a1':'a6']; raw_y = df.loc[:, 'class']\n",
    "x_test = pd.get_dummies(raw_x, columns = raw_x.columns).astype(int); y_test = pd.get_dummies(raw_y).astype(int)\n",
    "\n",
    "y_predicted = nn.compute_multiple_outputs(x_test)\n",
    "test_error = MSE()(y_test.to_numpy(), y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [0,0,0],\n",
    "    [1,1,1],\n",
    "    [1,2,3],\n",
    "    [2,3,4]\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    [0,0],\n",
    "    [1,1],\n",
    "    [1,2],\n",
    "    [2,3]\n",
    "])\n",
    "\n",
    "ds = Dataset(x, y)\n",
    "\n",
    "ds_1, ds_2 = ds.split(0.50, shuffle = False)\n",
    "\n",
    "print(ds_1.x, ds_1.y)\n",
    "\n",
    "print(ds_2.x, ds_2.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[[0,2], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams: dict[list] = {\n",
    "    'learning_rate': [0.1, 0.3, 0.5],\n",
    "    'regularization_term': [Tikhonov(0.1), Tikhonov(0.2)]\n",
    "}\n",
    "\n",
    "architectures = [\n",
    "    NeuralNetworkArchitecture([17, 6, 2], Sigmoid(), RandomUniform(0.3)),\n",
    "    NeuralNetworkArchitecture([17, 5, 2], Sigmoid(), RandomUniform(0.3))\n",
    "]\n",
    "\n",
    "grid = HyperparameterGrid(Backprop, hyperparams, architectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in grid:\n",
    "    print(x[0], x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'key_a': [123, 3.45, np.array([0,1,2]), 'hello'], 'key_back': [456, 789]}\n",
    "#print(d.values())\n",
    "#print([x for x in d.values()])\n",
    "\n",
    "import itertools\n",
    "\n",
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "c = [7,8,9]\n",
    "#for x in itertools.product(*d.values()): print(x)\n",
    "\n",
    "print(zip(a,b))\n",
    "\n",
    "d.get('key_b', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "inspect.signature(Backprop.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
