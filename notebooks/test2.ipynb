{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import annotations\n",
    "%matplotlib widget\n",
    "\n",
    "from typing import TypeAlias\n",
    "Vector: TypeAlias = np.ndarray  # A 1-D array\n",
    "Matrix: TypeAlias = np.ndarray  # A 2-D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerInitializationStrategy:\n",
    "    \"\"\"Parent class for initialization strategies of weights and biases in Layer.\"\"\"\n",
    "    def run(self, size_of_previous_layer: int, size_of_current_layer: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "        raise NotImplementedError(\"The 'run' method is implemented only in child classes\")\n",
    "\n",
    "class RandomUniform(LayerInitializationStrategy):\n",
    "    \"\"\"\n",
    "    Initialization strategy sampling weights and biases uniformly in a given interval.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    scale: float\n",
    "        The half-lenght of the interval [-scale, scale] from which values are sampled.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    run(self, size_of_previous_layer: int, size_of_current_layer: int) -> tuple[np.ndarray, np.ndarray]\n",
    "        Returns the tuple (random_weights, random_biases), where random_weights and random_biases are np.ndarrays of the appropriate shape.\n",
    "    \"\"\"\n",
    "    def __init__(self, scale_of_interval: float, center_of_interval: float = 0):\n",
    "        self.scale: float = scale_of_interval\n",
    "        self.center: float = center_of_interval\n",
    "        \n",
    "    \n",
    "    def run(self, size_of_previous_layer: int, size_of_current_layer: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "        random_weights: np.ndarray = np.random.uniform(\n",
    "            -self.scale + self.center, self.scale + self.center,\n",
    "            (size_of_previous_layer, size_of_current_layer)\n",
    "            )\n",
    "        random_biases: np.ndarray = np.random.uniform(\n",
    "            -self.scale + self.center, self.scale + self.center,\n",
    "            size_of_current_layer\n",
    "            )\n",
    "        return random_weights, random_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    \"\"\"Parent class for activation functions of neural nodes.\"\"\"\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError(\"The '__call__' method must be implemented in child classes\")\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError(\"The 'derivative' method must be implemented in child classes\")\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    \"\"\"Sigmoid activation function. f(x) = 1 / (1 + np.exp(-x))\"\"\"\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        sigmoid = self(x)  # Reuse the __call__ method to compute sigmoid\n",
    "        return sigmoid * (1 - sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A Layer component of a NeuralNetwork.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    unit_number: int\n",
    "        The number of nodes/units in the Layer.\n",
    "    init_strat: LayerInitializationStrategy\n",
    "        The initialization strategy for the weights and biases of the Layer\n",
    "    activation_function: ActivationFunction\n",
    "        The function.\n",
    "    \n",
    "    I should continue writing docstrings later\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, number_of_units: int,\n",
    "                 initialization_strategy: LayerInitializationStrategy,\n",
    "                 activation_function: ActivationFunction):\n",
    "        self.unit_number: int = number_of_units  # The number of nodes/units in the Layer.\n",
    "        self.init_strat: LayerInitializationStrategy = initialization_strategy\n",
    "        self.activation_function: ActivationFunction = activation_function\n",
    "        self.linear_output: Vector = None\n",
    "        self.output: Vector = None  # The values computed by the units based on the outputs of the previous layer. Stored for later backprop.\n",
    "        self.previous_layer: Layer = None  # The layer preceding the current one in the Neural Network. The NN should connect layers during initialization.\n",
    "        self.next_layer: Layer = None\n",
    "        self.weights: Matrix = None; self.biases: Vector = None  # Weights and biases connecting the layer with the previous layer of the neural network.\n",
    "        self.delta: Vector = None\n",
    "        \n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.weights, self.biases = self.init_strat.run(self.previous_layer.unit_number, self.unit_number)\n",
    "\n",
    "    def compute_output(self):\n",
    "        self.linear_output = np.dot(self.previous_layer.output, self.weights) + self.biases\n",
    "        self.output: Vector = self.activation_function(self.linear_output)\n",
    "        return self.output\n",
    "    \n",
    "    \n",
    "    # def compute_gradient(self)  # Should this function be here or in a later TrainingAlgorithm class?\n",
    "\n",
    "class InputLayer(Layer):\n",
    "    def __init__(self, number_of_units: int):\n",
    "        super().__init__(number_of_units, None, None)\n",
    "        # An input layer has no previous layer to connect to, so attributes referring to a previous layer are deleted.\n",
    "        del self.previous_layer, self.weights, self.biases, self.init_strat, self.activation_function\n",
    "    \n",
    "    def feed_input(self, value: Vector) -> None:\n",
    "        self.output: Vector = value\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        raise NotImplementedError(\"InputLayer does not require weight initialization.\")\n",
    "\n",
    "    def compute_output(self):\n",
    "        return self.output\n",
    "\n",
    "class HiddenLayer(Layer):\n",
    "    def backward(self):\n",
    "        self.delta = np.dot(self.next_layer.weights, self.next_layer.delta) * self.activation_function.derivative(self.linear_output)\n",
    "\n",
    "class OutputLayer(Layer):\n",
    "    def __init__(self, number_of_units: int,\n",
    "                 initialization_strategy: LayerInitializationStrategy,\n",
    "                 activation_function: ActivationFunction):\n",
    "        super().__init__(number_of_units, initialization_strategy, activation_function)\n",
    "        del self.next_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListOfArrays:\n",
    "    \"\"\"\n",
    "    An utility class for defining element-wise operations on lists containing heteromorphic np.ndarrays.\n",
    "    Useful for storing network weights and biases in MLP-architecture NeuralNetworks.\n",
    "    \"\"\"\n",
    "    def __init__(self, arrays: list[np.ndarray]):\n",
    "        self.arrays: list[np.ndarray] = arrays\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ListOfArrays{(self.arrays)}\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.arrays[index]\n",
    "\n",
    "    def __setitem__(self, index, value):\n",
    "        self.arrays[index] = value\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if not isinstance(other, ListOfArrays): raise TypeError(\"Operand is not a ListOfArrays\")\n",
    "        return ListOfArrays([x + y for x, y in zip(self.arrays, other.arrays)])\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if not isinstance(other, ListOfArrays): raise TypeError(\"Operand is not a ListOfArrays\")\n",
    "        return ListOfArrays([x - y for x, y in zip(self.arrays, other.arrays)])\n",
    "    \n",
    "    def __mul__(self, scalar: float):\n",
    "        return ListOfArrays([x * scalar for x in self.arrays])\n",
    "    \n",
    "    def __rmul__(self, scalar: float):\n",
    "        return self.__mul__(scalar)\n",
    "\n",
    "    def __truediv__(self, scalar: float):\n",
    "        return ListOfArrays([x / scalar for x in self.arrays])\n",
    "    \n",
    "    def __pow__(self, power: float):\n",
    "        return ListOfArrays([x**power for x in self.arrays])\n",
    "    \n",
    "    def sum(self) -> float:\n",
    "        return np.sum([np.sum(array) for array in self.arrays])\n",
    "    \n",
    "    def set_all_values_to(self, value: float) -> None:\n",
    "        for a in self.arrays:\n",
    "            a = value*np.ones(a.shape)\n",
    "\n",
    "\n",
    "class ListOfVectors(ListOfArrays):\n",
    "    def __init__(self, arrays: list[Vector]):\n",
    "        super().__init__(arrays)\n",
    "\n",
    "class ListOfMatrices(ListOfArrays):\n",
    "    pass\n",
    "    def __init__(self, arrays: list[Matrix]):\n",
    "        super().__init__(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers: list[Layer]):\n",
    "        self.layers: list[Layer] = layers\n",
    "        # Maybe here I should ensure that layers are correctly typed (layers[0] should be an InputLayer, layers[-1] an OutputLayer, all other layers should be HiddenLayer).\n",
    "        self.input_layer: InputLayer = layers[0]; self.hidden_layers: list[HiddenLayer] = layers[1: -1]; self.output_layer: OutputLayer = layers[-1]\n",
    "        self.layers_with_weights: list[Layer] = self.layers[1: ]\n",
    "        self.connect_layers()\n",
    "        self.initialize_weights()\n",
    "\n",
    "        #self.weights = ListOfMatrices([l.weights for l in self.layers_with_weights])\n",
    "        #self.biases = ListOfVectors([l.biases for l in self.layers_with_weights])\n",
    "    \n",
    "    def connect_layers(self) -> None:\n",
    "        for (i, layer) in enumerate(self.layers):\n",
    "            if not isinstance(layer, InputLayer): layer.previous_layer = self.layers[i - 1]\n",
    "            if not isinstance(layer, OutputLayer): layer.next_layer = self.layers[i + 1]\n",
    "\n",
    "    def initialize_weights(self) -> None:\n",
    "        for layer in self.layers_with_weights: layer.initialize_weights()\n",
    "    \n",
    "    def feed_input(self, value: np.ndarray) -> None:\n",
    "        self.input_layer.feed_input(value)\n",
    "\n",
    "    def activate_network(self) -> np.ndarray:\n",
    "        for i in range(len(self.layers)): self.layers[i].compute_output()\n",
    "        return self.output_layer.output\n",
    "    \n",
    "    def compute_output(self, value: np.ndarray) -> np.ndarray:\n",
    "        self.feed_input(value)\n",
    "        return self.activate_network()\n",
    "    \n",
    "    def backward(self) -> None:\n",
    "        for l in reversed(self.hidden_layers):\n",
    "            l.backward()\n",
    "    \n",
    "    def compute_multiple_outputs(self, x_data: pd.DataFrame | np.ndarray) -> np.ndarray[np.ndarray]:\n",
    "        if isinstance(x_data, pd.DataFrame): x_data = x_data.to_numpy()\n",
    "        outputs = np.array(\n",
    "            [\n",
    "                self.compute_output(x_data[i]) for i in range(len(x_data))\n",
    "            ]\n",
    "        )\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizationTerm:\n",
    "    def set_network(self, network: NeuralNetwork) -> None:\n",
    "        self.network = network\n",
    "\n",
    "    def __call__(self) -> float:\n",
    "        pass\n",
    "\n",
    "    def gradient(self) -> tuple[ListOfArrays, ListOfArrays]:\n",
    "        pass\n",
    "\n",
    "class NoRegularization(RegularizationTerm):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self) -> float:\n",
    "        return 0\n",
    "    \n",
    "    def gradient(self) -> tuple[ListOfArrays, ListOfArrays]:\n",
    "        layers = self.network.layers_with_weights\n",
    "        return ListOfArrays([np.zeros_like(l.weights) for l in layers]), ListOfArrays([np.zeros_like(l.biases) for l in layers])\n",
    "\n",
    "class Tikhonov(RegularizationTerm):\n",
    "    def __init__(self, penalty: float):\n",
    "        self.penalty: float = penalty\n",
    "        self.network: NeuralNetwork = None\n",
    "\n",
    "    def __call__(self) -> float:\n",
    "        layers: list[Layer] = self.network.layers_with_weights\n",
    "        weights_term = np.sum([np.sum(layer.weigths**2) for layer in layers])  # The sum of squares of all the weights in the NN.\n",
    "        biases_term = np.sum([np.sum(layer.biases**2) for layer in layers])\n",
    "        return self.penalty * (weights_term + biases_term) / 2\n",
    "\n",
    "    def gradient(self) -> tuple[ListOfArrays, ListOfArrays]:\n",
    "        layers: list[Layer] = self.network.layers_with_weights\n",
    "        gradient_on_weights: ListOfArrays = ListOfArrays([-self.penalty * l.weights for l in layers])\n",
    "        gradient_on_biases: ListOfArrays = ListOfArrays([-self.penalty * l.biases for l in layers])\n",
    "        return gradient_on_weights, gradient_on_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumRule:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class TrainingAlgorithm: pass\n",
    "\n",
    "class StoppingCondition:\n",
    "    def __init__(self):\n",
    "        self.alg: TrainingAlgorithm = None\n",
    "    \n",
    "    def set_alg(self, alg: TrainingAlgorithm) -> None:\n",
    "        self.alg = alg\n",
    "\n",
    "    @property\n",
    "    def is_satisfied(self) -> bool:\n",
    "        pass\n",
    "\n",
    "class ThresholdOnTrainingError(StoppingCondition):\n",
    "    def __init__(self, threshold: float, patience: int):\n",
    "        super().__init__()\n",
    "        self.threshold: float = threshold\n",
    "        self.patience: int = patience\n",
    "    \n",
    "    @property\n",
    "    def is_satisfied(self) -> bool:\n",
    "        current_training_error: float = self.alg.current_tr_err\n",
    "        if current_training_error < self.threshold:\n",
    "            self.consecutive_epochs += 1\n",
    "            return self.consecutive_epochs > self.patience\n",
    "        else:\n",
    "            self.consecutive_epochs = 0\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorFunction:\n",
    "    def __call__(self, y_data: np.ndarray, y_predicted: np.ndarray) -> float:\n",
    "        pass\n",
    "    \n",
    "    def simple_gradient(self, y_data: np.ndarray, y_predicted: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "class MSE(ErrorFunction):\n",
    "    def __call__(self, y_data: np.ndarray, y_predicted: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Returns the average over the dataset of the square euclidean distance between the training outputs and the predictions.\n",
    "        \"\"\"\n",
    "        num_patterns = 1 if y_data.ndim == 1 else len(y_data)\n",
    "        return 0.5 * np.sum((y_data - y_predicted)**2) / num_patterns\n",
    "    \n",
    "    def simple_gradient(self, y_data: np.ndarray, y_predicted: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns y_data - y_predicted. It is meant to be used on a single pattern at a time, during backpropagation.\n",
    "        \"\"\"\n",
    "        return (y_data - y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, x_data: Matrix, y_data: Matrix):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with input (x_data) and output (y_data).\n",
    "        \"\"\"\n",
    "        if x_data.ndim == 1 or y_data.ndim == 1:\n",
    "            raise ValueError(f\"x_data and y_data should be matrices, where each row represents a pattern and each column a feature, but got arguments of shape {x_data.shape} and {y_data.shape}\")\n",
    "\n",
    "        self.x: Matrix = x_data\n",
    "        self.y: Matrix = y_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of patterns in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves the input-output pair at the specified index.\n",
    "        \"\"\"\n",
    "        return self.x[index], self.y[index]  \n",
    "\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self, dataset: Dataset, minibatch_size: int = None, shuffle: bool = False):\n",
    "        self.dataset: Dataset = dataset\n",
    "        self.minibatch_size: int = minibatch_size or len(dataset)\n",
    "        self.shuffle: bool = shuffle\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        An iterator yielding minibatches\n",
    "        \"\"\"\n",
    "        indices = np.arange(len(self.dataset))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        for start in range(0, len(indices), self.minibatch_size):\n",
    "            minibatch_indices = indices[start:start + self.minibatch_size]\n",
    "            minibatch_x = self.dataset.x[minibatch_indices, :]\n",
    "            minibatch_y = self.dataset.y[minibatch_indices, :]\n",
    "            yield minibatch_x, minibatch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingAlgorithm:\n",
    "    def __init__(self, x_train: pd.DataFrame, y_train: pd.DataFrame, network: NeuralNetwork):\n",
    "        self.network: NeuralNetwork = network\n",
    "\n",
    "        if isinstance(x_train, pd.DataFrame): x_train = x_train.to_numpy()\n",
    "        if isinstance(y_train, pd.DataFrame): y_train = y_train.to_numpy()\n",
    "        self.training_set = Dataset(x_train, y_train)\n",
    "\n",
    "        self.current_tr_err: float = float('inf')\n",
    "\n",
    "        self.history: dict[list] = {'training error': []}\n",
    "        self.cache: dict = {}\n",
    "\n",
    "class Backprop(TrainingAlgorithm):\n",
    "    def __init__(self, x_train: pd.DataFrame, y_train: pd.DataFrame, network: NeuralNetwork,\n",
    "                 learning_rate: float,\n",
    "                 error_function: ErrorFunction,\n",
    "                 stopping_condition: StoppingCondition,\n",
    "                 regularization_term: RegularizationTerm = None,\n",
    "                 minibatch_size: int = None\n",
    "                 ):\n",
    "        super().__init__(x_train, y_train, network)\n",
    "\n",
    "        self.learning_rate: float = learning_rate\n",
    "\n",
    "        self.err_fun: ErrorFunction = error_function\n",
    "        \n",
    "        self.stop_cond: StoppingCondition = stopping_condition\n",
    "        self.stop_cond.set_alg(self)\n",
    "\n",
    "        self.weights_gradient: ListOfMatrices = ListOfMatrices([np.zeros_like(l.weights) for l in self.network.layers_with_weights])\n",
    "        self.biases_gradient: ListOfVectors = ListOfVectors([np.zeros_like(l.biases) for l in self.network.layers_with_weights])\n",
    "\n",
    "        self.regularization_term: RegularizationTerm = regularization_term or NoRegularization()\n",
    "        self.regularization_term.set_network(self.network)\n",
    "\n",
    "        self.minibatch_size: int = minibatch_size or len(self.training_set)\n",
    "        self.minibatch_generator: DataManager = DataManager(self.training_set, self.minibatch_size,\n",
    "                                                            shuffle = (self.minibatch_size != len(self.training_set))\n",
    "                                                            )\n",
    "        self.current_mb_size: int = None\n",
    "        self.current_mb_x: Matrix = None; self.current_mb_y: Matrix = None\n",
    "        \n",
    "    \n",
    "    def run(self, max_epochs: int) -> None:\n",
    "        epoch: int = 0\n",
    "        while epoch < max_epochs:\n",
    "            epoch += 1\n",
    "\n",
    "            for minibatch_x, minibatch_y in self.minibatch_generator:\n",
    "                self.update_minibatch_metadata(minibatch_x, minibatch_y)\n",
    "                self.update_gradients()\n",
    "                self.update_network_parameters()\n",
    "            \n",
    "            self.compute_training_error()\n",
    "            if self.stop_cond.is_satisfied: break\n",
    "\n",
    "    def update_minibatch_metadata(self, minibatch_x: Matrix, minibatch_y: Matrix):\n",
    "        self.current_mb_x = minibatch_x; self.current_mb_y = minibatch_y\n",
    "        self.current_mb_size = len(minibatch_x)\n",
    "    \n",
    "    def update_gradients(self):\n",
    "        self.reset_gradients()\n",
    "\n",
    "        for x, y in zip(self.current_mb_x, self.current_mb_y):\n",
    "            predicted_y = self.network.compute_output(x)\n",
    "            out_l = self.network.output_layer\n",
    "            out_l.delta = self.err_fun.simple_gradient(y, predicted_y)*out_l.activation_function.derivative(out_l.linear_output)\n",
    "            self.network.backward()\n",
    "            self.weights_gradient += ListOfMatrices([np.outer(l.previous_layer.output, l.delta) for l in self.network.layers_with_weights])\n",
    "            self.biases_gradient += ListOfVectors([l.delta for l in self.network.layers_with_weights])\n",
    "        self.weights_gradient /= self.current_mb_size; self.biases_gradient /= self.current_mb_size\n",
    "\n",
    "        self.add_regul_contribution()\n",
    "        self.add_momentum_contribution()\n",
    "    \n",
    "    def reset_gradients(self) -> None:\n",
    "        self.weights_gradient.set_all_values_to(0)\n",
    "        self.biases_gradient.set_all_values_to(0)\n",
    "    \n",
    "    def add_regul_contribution(self) -> None:\n",
    "        contribution_to_w, contribution_to_b = self.regularization_term.gradient()\n",
    "        self.weights_gradient += contribution_to_w; self.biases_gradient += contribution_to_b\n",
    "\n",
    "    def add_momentum_contribution(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def update_network_parameters(self) -> None:\n",
    "        factor = self.learning_rate * self.current_mb_size / len(self.training_set)\n",
    "\n",
    "        for i, l in enumerate(self.network.layers_with_weights):\n",
    "            l.weights += factor * self.weights_gradient[i]\n",
    "            l.biases += factor * self.biases_gradient[i]\n",
    "        #self.network.weights += factor * self.weights_gradient\n",
    "        #self.network.biases += factor * self.biases_gradient\n",
    "        \n",
    "\n",
    "    def compute_training_error(self) -> None:\n",
    "        y_prediction = self.network.compute_multiple_outputs(self.current_mb_x)\n",
    "        self.current_tr_err = self.err_fun(self.current_mb_y, y_prediction)\n",
    "        self.history['training error'] += [self.current_tr_err]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n",
      "[2.  2.5 5. ]\n",
      "[0.88079708 0.92414182 0.99330715]\n",
      "[1.73577357 7.41481669]\n",
      "[0.85014943 0.9993981 ]\n",
      "[ 0.01909027 -0.00060118]\n",
      "[ 0.001815    0.00255016 -0.00012691]\n"
     ]
    }
   ],
   "source": [
    "# Some fake training data\n",
    "x = np.array(\n",
    "    [[1, 0],\n",
    "     [1, 0],\n",
    "     [1, 0]], dtype = float\n",
    ")\n",
    "y = np.array(\n",
    "    [[1, 0],\n",
    "     [1, 0],\n",
    "     [1, 0]], dtype = float\n",
    ")\n",
    "\n",
    "\n",
    "# Define and initialize the nn manually\n",
    "layers = [InputLayer(2), HiddenLayer(3, RandomUniform(0.0), Sigmoid()), OutputLayer(2, RandomUniform(0.0), Sigmoid())]\n",
    "nn = NeuralNetwork(layers)\n",
    "\n",
    "# Initialize layers\n",
    "nn.hidden_layers[0].weights = np.array(\n",
    "    [[1, 0.5, 2],\n",
    "    [-1, 0, -1.5]], dtype = float\n",
    ")\n",
    "nn.hidden_layers[0].biases = np.array([1, 2, 3], dtype=float)\n",
    "\n",
    "nn.output_layer.weights = np.array(\n",
    "    [[1, 3],\n",
    "     [2, 3],\n",
    "     [-1, 0]], dtype = float\n",
    ")\n",
    "nn.output_layer.biases = np.array([0,2], dtype=float)\n",
    "\n",
    "# Test forward pass\n",
    "nn.compute_output(x[0])\n",
    "\n",
    "print(nn.input_layer.output)\n",
    "print(nn.hidden_layers[0].linear_output)\n",
    "print(nn.hidden_layers[0].output)\n",
    "print(nn.output_layer.linear_output)\n",
    "print(nn.output_layer.output)\n",
    "\n",
    "\n",
    "# Test packward pass\n",
    "nn.output_layer.delta = (y[0] - nn.compute_output(x[0])) * nn.output_layer.activation_function.derivative(nn.output_layer.linear_output)\n",
    "nn.backward()\n",
    "print(nn.output_layer.delta)\n",
    "print(nn.hidden_layers[0].delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_alg = Backprop(x, y, nn, 200.0, MSE(), ThresholdOnTrainingError(0.00001, 10), regularization_term = Tikhonov(0.000), minibatch_size = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListOfArrays[array([[ 0.36299938,  0.51003272, -0.02538264],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 3.3629306 , -0.1059028 ],\n",
      "       [ 3.52842316, -0.11111436],\n",
      "       [ 3.79250011, -0.11943047]])]\n",
      "[[ 0.36299938  0.51003272 -0.02538264]\n",
      " [ 0.          0.         -0.        ]]\n",
      "[[ 3.58340937 -0.11284595]\n",
      " [ 3.70829744 -0.11677882]\n",
      " [ 3.79117874 -0.11938886]]\n"
     ]
    }
   ],
   "source": [
    "old_weights = ListOfArrays([np.copy(nn.hidden_layers[0].weights), np.copy(nn.output_layer.weights)])\n",
    "old_biases = ListOfArrays([np.copy(nn.hidden_layers[0].biases), np.copy(nn.output_layer.biases)])\n",
    "\n",
    "# Test one epoch update\n",
    "training_alg.run(1)\n",
    "new_weights = ListOfArrays([np.copy(nn.hidden_layers[0].weights), np.copy(nn.output_layer.weights)])\n",
    "new_biases = ListOfArrays([np.copy(nn.hidden_layers[0].biases), np.copy(nn.output_layer.biases)])\n",
    "\n",
    "\n",
    "\n",
    "print(new_weights - old_weights)\n",
    "\n",
    "\n",
    "print(\n",
    "    training_alg.learning_rate * np.outer(nn.input_layer.output, nn.hidden_layers[0].delta)\n",
    ")\n",
    "print(\n",
    "    training_alg.learning_rate * np.outer(nn.hidden_layers[0].output, nn.output_layer.delta)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
