{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerInitializationStrategy:\n",
    "    \"\"\"Parent class for initialization strategies of weights and biases in Layer.\"\"\"\n",
    "    def run(self, size_of_previous_layer: int, size_of_current_layer: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "        raise NotImplementedError(\"The 'run' method is implemented only in child classes\")\n",
    "\n",
    "class RandomUniform(LayerInitializationStrategy):\n",
    "    \"\"\"\n",
    "    Initialization strategy sampling weights and biases uniformly in a given interval.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    scale: float\n",
    "        The half-lenght of the interval [-scale, scale] from which values are sampled.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    run(self, size_of_previous_layer: int, size_of_current_layer: int) -> tuple[np.ndarray, np.ndarray]\n",
    "        Returns the tuple (random_weights, random_biases), where random_weights and random_biases are np.ndarrays of the appropriate shape.\n",
    "    \"\"\"\n",
    "    def __init__(self, scale_of_interval: float, center_of_interval: float = 0):\n",
    "        self.scale: float = scale_of_interval\n",
    "        self.center: float = center_of_interval\n",
    "    \n",
    "    def run(self, size_of_previous_layer: int, size_of_current_layer: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "        random_weights: np.ndarray = np.random.uniform(\n",
    "            -self.scale + self.center, self.scale + self.center,\n",
    "            (size_of_previous_layer, size_of_current_layer)\n",
    "            )\n",
    "        random_biases: np.ndarray = np.random.uniform(\n",
    "            -self.scale + self.center, self.scale + self.center,\n",
    "            size_of_current_layer\n",
    "            )\n",
    "        return random_weights, random_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    \"\"\"Parent class for activation functions of neural nodes.\"\"\"\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError(\"The '__call__' method must be implemented in child classes\")\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError(\"The 'derivative' method must be implemented in child classes\")\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    \"\"\"Sigmoid activation function. f(x) = 1 / (1 + np.exp(-x))\"\"\"\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        sigmoid = self(x)  # Reuse the __call__ method to compute sigmoid\n",
    "        return sigmoid * (1 - sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A Layer component of a NeuralNetwork.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    unit_number: int\n",
    "        The number of nodes/units in the Layer.\n",
    "    init_strat: LayerInitializationStrategy\n",
    "        The initialization strategy for the weights and biases of the Layer\n",
    "    activation_function: ActivationFunction\n",
    "        The function.\n",
    "    \n",
    "    I should continue writing docstrings later\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, number_of_units: int,\n",
    "                 initialization_strategy: LayerInitializationStrategy,\n",
    "                 activation_function: ActivationFunction):\n",
    "        self.unit_number: int = number_of_units  # The number of nodes/units in the Layer.\n",
    "        self.init_strat: LayerInitializationStrategy = initialization_strategy\n",
    "        self.activation_function: ActivationFunction = activation_function\n",
    "        self.linear_output: np.ndarray = None\n",
    "        self.output: np.ndarray = None  # The values computed by the units based on the outputs of the previous layer. Stored for later backprop.\n",
    "        self.previous_layer: Layer = None  # The layer preceding the current one in the Neural Network. The NN should connect layers during initialization.\n",
    "        self.next_layer: Layer = None\n",
    "        self.weights: np.ndarray = None; self.biases: np.ndarray = None  # Weights and biases connecting the layer with the previous layer of the neural network.\n",
    "        self.delta: np.ndarray = None\n",
    "        \n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.weights, self.biases = self.init_strat.run(self.previous_layer.unit_number, self.unit_number)\n",
    "\n",
    "    def compute_output(self):\n",
    "        self.linear_output = np.dot(self.previous_layer.output, self.weights) + self.biases\n",
    "        self.output: np.ndarray = self.activation_function(self.linear_output)\n",
    "        return self.output\n",
    "    \n",
    "    # def compute_gradient(self)  # Should this function be here or in a later TrainingAlgorithm class?\n",
    "\n",
    "class InputLayer(Layer):\n",
    "    def __init__(self, number_of_units: int):\n",
    "        super().__init__(number_of_units, None, None)\n",
    "        # An input layer has no previous layer to connect to, so attributes referring to a previous layer are deleted.\n",
    "        del self.previous_layer, self.weights, self.biases, self.init_strat, self.activation_function\n",
    "    \n",
    "    def feed_input(self, value: np.ndarray) -> None:\n",
    "        self.output: np.ndarray = value\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        raise NotImplementedError(\"InputLayer does not require weight initialization.\")\n",
    "\n",
    "    def compute_output(self):\n",
    "        return self.output\n",
    "\n",
    "class HiddenLayer(Layer):\n",
    "    pass\n",
    "\n",
    "class OutputLayer(Layer):\n",
    "    def __init__(self, number_of_units: int,\n",
    "                 initialization_strategy: LayerInitializationStrategy,\n",
    "                 activation_function: ActivationFunction):\n",
    "        super().__init__(number_of_units, initialization_strategy, activation_function)\n",
    "        del self.next_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers: list[Layer]):\n",
    "        self.layers: list[Layer] = layers\n",
    "        # Maybe here I should ensure that layers are correctly typed (layers[0] should be an InputLayer, layers[-1] an OutputLayer, all other layers should be HiddenLayer).\n",
    "        self.input_layer: InputLayer = layers[0]; self.hidden_layers: list[HiddenLayer] = layers[1: -1]; self.output_layer: OutputLayer = layers[-1]\n",
    "        self.layers_with_weights: list[Layer] = self.layers[1: ]\n",
    "        self.connect_layers()\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def connect_layers(self) -> None:\n",
    "        for (i, layer) in enumerate(self.layers):\n",
    "            if not isinstance(layer, InputLayer): layer.previous_layer = self.layers[i - 1]\n",
    "            if not isinstance(layer, OutputLayer): layer.next_layer = self.layers[i + 1]\n",
    "\n",
    "    def initialize_weights(self) -> None:\n",
    "        \n",
    "        for layer in self.layers_with_weights: layer.initialize_weights()\n",
    "    \n",
    "    def feed_input(self, value: np.ndarray) -> None:\n",
    "        self.input_layer.feed_input(value)\n",
    "\n",
    "    def activate_network(self) -> np.ndarray:\n",
    "        for i in range(len(self.layers)): self.layers[i].compute_output()\n",
    "        return self.output_layer.output\n",
    "    \n",
    "    def compute_output(self, value: np.ndarray) -> np.ndarray:\n",
    "        self.feed_input(value)\n",
    "        return self.activate_network()\n",
    "    \n",
    "    def compute_multiple_outputs(self, x_data: pd.DataFrame | np.ndarray) -> np.ndarray[np.ndarray]:\n",
    "        if isinstance(x_data, pd.DataFrame): x_data = x_data.to_numpy()\n",
    "        outputs = np.array(\n",
    "            [\n",
    "                self.compute_output(x_data[i]) for i in range(len(x_data))\n",
    "            ]\n",
    "        )\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [InputLayer(2), HiddenLayer(3, RandomUniform(0.0), Sigmoid()), OutputLayer(2, RandomUniform(0.0), Sigmoid())]\n",
    "nn = NeuralNetwork(layers)\n",
    "\n",
    "# Initialize layers\n",
    "nn.hidden_layers[0].weights = np.array(\n",
    "    [[1, 0.5, 2],\n",
    "    [-1, 0, -1.5]], dtype = float\n",
    ")\n",
    "nn.hidden_layers[0].biases = np.array([1, 2, 3], dtype=float)\n",
    "\n",
    "nn.output_layer.weights = np.array(\n",
    "    [[1, 3],\n",
    "     [2, 3],\n",
    "     [-1, 0]], dtype = float\n",
    ")\n",
    "nn.output_layer.biases = np.array([0,2], dtype=float)\n",
    "\n",
    "\n",
    "# Some fake training data\n",
    "x = np.array(\n",
    "    [[1, 0]], dtype = float\n",
    ")\n",
    "y = np.array(\n",
    "    [[1,0]], dtype = float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n",
      "[2.  2.5 5. ]\n",
      "[0.88079708 0.92414182 0.99330715]\n",
      "[1.73577357 7.41481669]\n",
      "[0.85014943 0.9993981 ]\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "nn.compute_output(x[0])\n",
    "\n",
    "print(nn.input_layer.output)\n",
    "print(nn.hidden_layers[0].linear_output)\n",
    "print(nn.hidden_layers[0].output)\n",
    "print(nn.output_layer.linear_output)\n",
    "print(nn.output_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    def __call__(self, y_data: np.ndarray, y_predicted: np.ndarray) -> float:\n",
    "        pass\n",
    "    \n",
    "    def simple_gradient(self, y_data: np.ndarray, y_predicted: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "class MSE(LossFunction):\n",
    "    def __call__(self, y_data: np.ndarray, y_predicted: np.ndarray) -> float:\n",
    "        return 0.5 * np.sum((y_data - y_predicted)**2) / len(y_data)\n",
    "    \n",
    "    def simple_gradient(self, y_data: np.ndarray, y_predicted: np.ndarray) -> np.ndarray:\n",
    "        return (y_data - y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListOfArrays:\n",
    "    \"\"\"\n",
    "    An utility class for defining operations on lists containing hetero-shaped np.ndarrays.\n",
    "    Useful for storing network weights and biases in MLP-architecture NeuralNetworks.\n",
    "    \"\"\"\n",
    "    def __init__(self, arrays: list[np.array]):\n",
    "        self.arrays: list[np.array] = arrays\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ListOfArrays{(self.arrays)}\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.arrays[index]\n",
    "\n",
    "    def __setitem__(self, index, value):\n",
    "        self.arrays[index] = value\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if not isinstance(other, ListOfArrays): raise TypeError(\"Operand is not a ListOfArrays\")\n",
    "        return ListOfArrays([x + y for x, y in zip(self.arrays, other.arrays)])\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if not isinstance(other, ListOfArrays): raise TypeError(\"Operand is not a ListOfArrays\")\n",
    "        return ListOfArrays([x - y for x, y in zip(self.arrays, other.arrays)])\n",
    "    \n",
    "    def __mul__(self, scalar):\n",
    "        return ListOfArrays([x * scalar for x in self.arrays])\n",
    "    \n",
    "    def __truediv__(self, scalar):\n",
    "        return ListOfArrays([x / scalar for x in self.arrays])\n",
    "    \n",
    "    def __pow__(self, power):\n",
    "        return ListOfArrays([x**power for x in self.arrays])\n",
    "    \n",
    "    def sum(self) -> float:\n",
    "        return np.sum([np.sum(array) for array in self.arrays])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizationTerm:\n",
    "    def set_network(self, network: NeuralNetwork) -> None:\n",
    "        self.network = network\n",
    "\n",
    "    def __call__(self) -> float:\n",
    "        pass\n",
    "\n",
    "    def gradient(self) -> tuple[ListOfArrays, ListOfArrays]:\n",
    "        pass\n",
    "\n",
    "class NoRegularization(RegularizationTerm):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self) -> float:\n",
    "        return 0\n",
    "    \n",
    "    def gradient(self) -> tuple[ListOfArrays, ListOfArrays]:\n",
    "        layers = self.network.layers_with_weights\n",
    "        return ListOfArrays([np.zeros_like(l.weights) for l in layers]), ListOfArrays([np.zeros_like(l.biases) for l in layers])\n",
    "\n",
    "class Tikhonov(RegularizationTerm):\n",
    "    def __init__(self, penalty: float):\n",
    "        self.penalty: float = penalty\n",
    "        self.network: NeuralNetwork = None\n",
    "\n",
    "    def __call__(self) -> float:\n",
    "        layers: list[Layer] = self.network.layers_with_weights\n",
    "        weights_term = np.sum([np.sum(layer.weigths**2) for layer in layers])  # The sum of squares of all the weights in the NN.\n",
    "        biases_term = np.sum([np.sum(layer.biases**2) for layer in layers])\n",
    "        return self.penalty * (weights_term + biases_term) / 2\n",
    "\n",
    "    def gradient(self) -> tuple[ListOfArrays, ListOfArrays]:\n",
    "        layers: list[Layer] = self.network.layers_with_weights\n",
    "        gradient_on_weights: ListOfArrays = ListOfArrays([-self.penalty * l.weights for l in layers])\n",
    "        gradient_on_biases: ListOfArrays = ListOfArrays([-self.penalty * l.biases for l in layers])\n",
    "        return gradient_on_weights, gradient_on_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingCondition:\n",
    "    def is_satisfied(self, loss: float, gradients: np.ndarray = None) -> bool:\n",
    "        raise NotImplementedError(\"This method is only implemented in child classes\")\n",
    "\n",
    "class ThresholdOnLoss(StoppingCondition):\n",
    "    def __init__(self, threshold: float, patience: int):\n",
    "        self.threshold = threshold; self.patience = patience\n",
    "        self.consecutive_epochs = 0\n",
    "    \n",
    "    def is_satisfied(self, loss: float, gradients: np.ndarray = None) -> bool:\n",
    "        if loss < self.threshold:\n",
    "            self.consecutive_epochs += 1\n",
    "            return self.consecutive_epochs > self.patience\n",
    "        else:\n",
    "            self.consecutive_epochs = 0\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorFunction:\n",
    "    def __call__(self, y_data: np.ndarray, y_predicted: np.ndarray) -> float:\n",
    "        pass\n",
    "    \n",
    "    def simple_gradient(self, y_data: np.ndarray, y_predicted: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "class MSE(ErrorFunction):\n",
    "    def __call__(self, y_data: np.ndarray, y_predicted: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Returns the average over the dataset of the square euclidean distance between the training outputs and the predictions.\n",
    "        \"\"\"\n",
    "        num_patterns = 1 if y_data.ndim == 1 else len(y_data)\n",
    "        return 0.5 * np.sum((y_data - y_predicted)**2) / num_patterns\n",
    "    \n",
    "    def simple_gradient(self, y_data: np.ndarray, y_predicted: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns y_data - y_predicted. It is meant to be used on a single pattern at a time, during backpropagation.\n",
    "        \"\"\"\n",
    "        return (y_data - y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingAlgorithm:\n",
    "    def __init__(self, x_data: pd.DataFrame, y_data: pd.DataFrame, network: NeuralNetwork):\n",
    "        self.x: np.ndarray = x_data.to_numpy() if isinstance(x_data, pd.DataFrame) else x_data\n",
    "        self.y: np.ndarray = y_data.to_numpy() if isinstance(y_data, pd.DataFrame) else y_data\n",
    "        self.network: NeuralNetwork = network\n",
    "\n",
    "        self.history: dict[list] = {'training error': []}\n",
    "        self.cache: dict = {}\n",
    "\n",
    "class Backpropagation(TrainingAlgorithm):\n",
    "    def __init__(self, x_data: pd.DataFrame, y_data: pd.DataFrame, network: NeuralNetwork,\n",
    "                 learning_rate: float,\n",
    "                 error_function: ErrorFunction, stopping_condition: StoppingCondition,\n",
    "                 regularization_term: RegularizationTerm = None, batch_size: int = None\n",
    "                 ):\n",
    "        super().__init__(x_data, y_data, network)\n",
    "        self.learning_rate: float = learning_rate\n",
    "        self.err_fun: ErrorFunction = error_function; self.stop_cond: StoppingCondition = stopping_condition\n",
    "\n",
    "\n",
    "        # The gradients of the loss function (= error function + regularization term) with respect to the network weights and biases.\n",
    "        # Used for updating the network parameters after each training epoch.\n",
    "        self.weights_gradient: ListOfArrays = None; self.biases_gradient: ListOfArrays = None\n",
    "        \n",
    "\n",
    "        \n",
    "        self.regularization_term: RegularizationTerm = regularization_term or NoRegularization()\n",
    "        self.regularization_term.set_network(self.network)\n",
    "\n",
    "        self.batch_size: int = batch_size or len(self.x)\n",
    "\n",
    "\n",
    "    def compute_loss(self, x: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Computes the loss of the current network over the given data.\n",
    "        Such data may be the whole training data (self.x, self.y) or may be a mini-batch.\n",
    "\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: np.ndarray\n",
    "            The input training (mini)batch data.\n",
    "        y: np.ndarray\n",
    "            The output training (mini)batch data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "            Training error + Regularization penalty term. \n",
    "        \"\"\"\n",
    "        return self.compute_training_error(x, y) + self.regularization_term()\n",
    "    \n",
    "    def compute_training_error(self, x: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Computes the error of the current network over the given data.\n",
    "        Such data may be the whole training data (self.x, self.y) or may be a mini-batch.\n",
    "        Such error includes no regularization penalty term.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: np.ndarray\n",
    "            The input training (mini)batch data.\n",
    "        y: np.ndarray\n",
    "            The output training (mini)batch data.        \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        error: float\n",
    "            Average of single-pattern errors over the dataset. \n",
    "        \"\"\"\n",
    "        network_outputs: np.ndarray = self.network.compute_multiple_outputs(x)\n",
    "        return self.err_fun(y, network_outputs)\n",
    "    \n",
    "    def update_network_parameters(self, current_minibatch_size: int) -> None:\n",
    "        \"\"\"\n",
    "        NOTE TO SELF: add the momentum term's contribution here later.\n",
    "        Updates the weights and biases of the NeuralNetwork based on the computed gradients and the learning rate.\n",
    "        \"\"\"\n",
    "        factor: float = self.learning_rate * current_minibatch_size / len(self.x)\n",
    "        # Note that weights and biases are stored in each individual layer, so an iteration over\n",
    "        # layers is necessary. NeuralNetwork has no weights: ListOfArrays attribute, so we can't just\n",
    "        # write self.network.weights += ... * self.weights_gradient\n",
    "        for i, l in enumerate(self.network.layers_with_weights):\n",
    "            l.weights += factor * self.weights_gradient[i]   \n",
    "            l.biases += factor * self.biases_gradient[i]\n",
    "    \n",
    "    def run(self, max_epochs: int) -> None:\n",
    "        \"\"\"\n",
    "        Train the NeuralNetwork according to this strategy.\n",
    "        \"\"\"\n",
    "        epoch: int = 0; tr_err: float = float('inf')\n",
    "        indices = np.arange(len(self.x))\n",
    "        while not self.stop_cond.is_satisfied(tr_err, None) and epoch < max_epochs:\n",
    "            epoch += 1; tr_err = 0\n",
    "\n",
    "            # Shuffle the data\n",
    "            np.random.shuffle(indices)\n",
    "            shuffled_x = self.x[indices]; shuffled_y = self.y[indices]\n",
    "\n",
    "            for start in range(0, len(self.x), self.batch_size):  # For loop over the minibatches.\n",
    "                end = start + self.batch_size\n",
    "                minibatch_x = shuffled_x[start:end]; minibatch_y = shuffled_y[start:end]\n",
    "                current_minibatch_size = len(minibatch_x)\n",
    "\n",
    "                self.update_gradients(minibatch_x, minibatch_y, cache_error = True)  # NOTE define this function.\n",
    "                tr_err += self.cache['minibatch_training_error'] * current_minibatch_size / len(self.x)  # Add the error of the minibatch to the whole-batch error.\n",
    "                self.update_network_parameters(current_minibatch_size)\n",
    "\n",
    "\n",
    "            self.history['training error'] += [tr_err]\n",
    "    \n",
    "    def update_gradients(self, x, y, cache_error: bool) -> None:\n",
    "        # Set gradients to 0.\n",
    "        self.reset_gradients()\n",
    "        if cache_error: self.cache['minibatch_training_error'] = 0\n",
    "        # Set gradients to the mean of single-pattern gradients of the error (without regularization) function.\n",
    "        for pattern_x, pattern_y in zip(x,y):\n",
    "            self.add_single_pattern_gradients_contribution(pattern_x, pattern_y, cache_error)\n",
    "        current_minibatch_size: int = len(x)\n",
    "        self.weights_gradient /= current_minibatch_size; self.biases_gradient /= current_minibatch_size\n",
    "        if cache_error: self.cache['minibatch_training_error'] /= current_minibatch_size\n",
    "        # Add the regularization term contribution.\n",
    "        w_reg_term, b_reg_term = self.regularization_term.gradient()\n",
    "        self.weights_gradient += w_reg_term; self.biases_gradient += b_reg_term\n",
    "    \n",
    "    def reset_gradients(self) -> None:\n",
    "        \"\"\"\n",
    "        Sets self.weights_gradient and self.biases_gradient to a list of appropriately-shaped arrays with all zero entries.\n",
    "        \"\"\"\n",
    "        lwws: list[Layer] = self.network.layers_with_weights\n",
    "        self.weights_gradient = ListOfArrays([np.zeros_like(l.weights) for l in lwws])\n",
    "        self.biases_gradient = ListOfArrays([np.zeros_like(l.biases) for l in lwws])\n",
    "\n",
    "    def add_single_pattern_gradients_contribution(self, pattern_x: np.ndarray, pattern_y: np.ndarray, cache_error: bool) -> None:\n",
    "        predicted_y: np.ndarray = self.network.compute_output(pattern_x)\n",
    "        if cache_error: self.cache['minibatch_training_error'] += self.err_fun(pattern_y, predicted_y)\n",
    "        self.backpropagate(pattern_y)\n",
    "\n",
    "        w_term = ListOfArrays([np.outer(l.previous_layer.output, l.delta) for l in self.network.layers_with_weights])\n",
    "        b_term = ListOfArrays([l.delta for l in self.network.layers_with_weights])\n",
    "\n",
    "        self.weights_gradient += w_term; self.biases_gradient += b_term\n",
    "    \n",
    "    def backpropagate(self, pattern_y: np.ndarray) -> None:\n",
    "        out_l: OutputLayer = self.network.output_layer; hid_ls: list[HiddenLayer] = self.network.hidden_layers  # Give short, convenient names to the OutputLayer and the HiddenLayers.\n",
    "\n",
    "        out_l.delta = out_l.activation_function.derivative(out_l.linear_output) * self.err_fun.simple_gradient(pattern_y, out_l.output)  # Compute delta for the OutputLayer.\n",
    "        for l in reversed(hid_ls):  # For each HiddenLayer, starting from the one closest to the OutputLayer and proceeding backwards:\n",
    "            l.delta = l.activation_function.derivative(l.linear_output) * np.dot(l.next_layer.weights, l.next_layer.delta)  # Compute delta for the HiddenLayer.\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_alg = Backpropagation(x, y, nn, 200.0, MSE(), ThresholdOnLoss(0.00001, 10), regularization_term = Tikhonov(0.000), batch_size = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01909027 -0.00060118]\n",
      "[ 0.001815    0.00255016 -0.00012691]\n"
     ]
    }
   ],
   "source": [
    "# Test backward pass\n",
    "training_alg.backpropagate(y[0])\n",
    "print(nn.output_layer.delta)\n",
    "print(nn.hidden_layers[0].delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_weights = ListOfArrays([np.copy(nn.hidden_layers[0].weights), np.copy(nn.output_layer.weights)])\n",
    "old_biases = ListOfArrays([np.copy(nn.hidden_layers[0].biases), np.copy(nn.output_layer.biases)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test one epoch update\n",
    "training_alg.run(1)\n",
    "new_weights = ListOfArrays([nn.hidden_layers[0].weights, nn.output_layer.weights])\n",
    "new_biases = ListOfArrays([nn.hidden_layers[0].biases, nn.output_layer.biases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListOfArrays[array([[ 0.36299938,  0.51003272, -0.02538264],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 3.3629306 , -0.1059028 ],\n",
      "       [ 3.52842316, -0.11111436],\n",
      "       [ 3.79250011, -0.11943047]])]\n"
     ]
    }
   ],
   "source": [
    "print(new_weights - old_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.36299938  0.51003272 -0.02538264]\n",
      " [ 0.          0.         -0.        ]]\n",
      "[[ 3.3629306  -0.1059028 ]\n",
      " [ 3.52842316 -0.11111436]\n",
      " [ 3.79250011 -0.11943047]]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    training_alg.learning_rate * np.outer(nn.input_layer.output, nn.hidden_layers[0].delta)\n",
    ")\n",
    "print(\n",
    "    training_alg.learning_rate * np.outer(nn.hidden_layers[0].output, nn.output_layer.delta)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_prod_for_1010 = ListOfArrays(\n",
    "    [np.copy(np.outer(nn.input_layer.output, nn.hidden_layers[0].delta)),\n",
    "    np.copy(np.outer(nn.hidden_layers[0].output, nn.output_layer.delta))]\n",
    ")\n",
    "deltas_for_1010 = ListOfArrays(\n",
    "    [np.copy(nn.hidden_layers[0].delta),\n",
    "    np.copy(nn.output_layer.delta)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [InputLayer(2), HiddenLayer(3, RandomUniform(0.0), Sigmoid()), OutputLayer(2, RandomUniform(0.0), Sigmoid())]\n",
    "nn = NeuralNetwork(layers)\n",
    "\n",
    "# Initialize layers\n",
    "nn.hidden_layers[0].weights = np.array(\n",
    "    [[1, 0.5, 2],\n",
    "    [-1, 0, -1.5]], dtype = float\n",
    ")\n",
    "nn.hidden_layers[0].biases = np.array([1, 2, 3], dtype=float)\n",
    "\n",
    "nn.output_layer.weights = np.array(\n",
    "    [[1, 3],\n",
    "     [2, 3],\n",
    "     [-1, 0]], dtype = float\n",
    ")\n",
    "nn.output_layer.biases = np.array([0,2], dtype=float)\n",
    "\n",
    "\n",
    "# Some fake training data, now with more data\n",
    "x = np.array(\n",
    "    [[1, 0],\n",
    "     [1, 0],\n",
    "     [-1, 3]], dtype = float\n",
    ")\n",
    "y = np.array(\n",
    "    [[1, 0],\n",
    "     [1, 0],\n",
    "     [2, 3]], dtype = float\n",
    ")\n",
    "\n",
    "training_alg = Backpropagation(x, y, nn, 200.0, MSE(), ThresholdOnLoss(0.00001, 10), regularization_term = Tikhonov(0.000), batch_size = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00976832  0.05559372 -0.00445349]\n"
     ]
    }
   ],
   "source": [
    "nn.compute_output(x[2])\n",
    "training_alg.backpropagate(y[2])\n",
    "print(nn.hidden_layers[0].delta)\n",
    "tensor_prod_for_m1323 = ListOfArrays(\n",
    "    [np.copy(np.outer(nn.input_layer.output, nn.hidden_layers[0].delta)),\n",
    "    np.copy(np.outer(nn.hidden_layers[0].output, nn.output_layer.delta))]\n",
    ")\n",
    "deltas_for_m1323 = ListOfArrays(\n",
    "    [np.copy(nn.hidden_layers[0].delta),\n",
    "    np.copy(nn.output_layer.delta)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListOfArrays[array([[-0.40922172, -3.36622628,  0.27997783],\n",
      "       [ 1.95366392, 11.11874428, -0.89069877]]), array([[ 2.73682987e+00, -7.67934138e-03],\n",
      "       [ 1.08834501e+01,  1.01064497e+00],\n",
      "       [ 2.83419858e+00, -4.07301575e-02]])]\n",
      "ListOfArrays[array([[-0.40922172, -3.36622628,  0.27997783],\n",
      "       [ 1.95366392, 11.11874428, -0.89069877]]), array([[ 2.73682987e+00, -7.67934138e-03],\n",
      "       [ 1.08834501e+01,  1.01064497e+00],\n",
      "       [ 2.83419858e+00, -4.07301575e-02]])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ListOfArrays[array([[ 0.00000000e+00,  0.00000000e+00,  1.66533454e-16],\n",
       "       [ 0.00000000e+00,  0.00000000e+00, -1.11022302e-16]]), array([[-4.44089210e-16, -1.56125113e-17],\n",
       "       [ 0.00000000e+00, -2.22044605e-16],\n",
       "       [ 0.00000000e+00,  0.00000000e+00]])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_alg = Backpropagation(x, y, nn, 200.0, MSE(), ThresholdOnLoss(0.00001, 10), regularization_term = Tikhonov(0.000), batch_size = None)\n",
    "old_weights = ListOfArrays([np.copy(nn.hidden_layers[0].weights), np.copy(nn.output_layer.weights)])\n",
    "old_biases = ListOfArrays([np.copy(nn.hidden_layers[0].biases), np.copy(nn.output_layer.biases)])\n",
    "\n",
    "# Test one epoch update\n",
    "training_alg.run(1)\n",
    "new_weights = ListOfArrays([nn.hidden_layers[0].weights, nn.output_layer.weights])\n",
    "new_biases = ListOfArrays([nn.hidden_layers[0].biases, nn.output_layer.biases])\n",
    "\n",
    "print(new_weights - old_weights)\n",
    "print(\n",
    "     ( tensor_prod_for_1010 * 2/3 + tensor_prod_for_m1323 * 1/3)*training_alg.learning_rate \n",
    ")\n",
    "(new_weights - old_weights) - ( tensor_prod_for_1010 * 2/3 + tensor_prod_for_m1323 * 1/3)*training_alg.learning_rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListOfArrays[array([0.00000000e+00, 0.00000000e+00, 1.11022302e-16]), array([0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "print((new_biases - old_biases) - ( deltas_for_1010 * 2/3 + deltas_for_m1323 * 1/3)*training_alg.learning_rate  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingAlgorithm:\n",
    "    def __init__(self, x_data: pd.DataFrame, y_data: pd.DataFrame, network: NeuralNetwork):\n",
    "        self.x: np.ndarray = x_data.to_numpy() if isinstance(x_data, pd.DataFrame) else x_data\n",
    "        self.y: np.ndarray = y_data.to_numpy() if isinstance(y_data, pd.DataFrame) else y_data\n",
    "        self.network: NeuralNetwork = network\n",
    "\n",
    "        self.history: dict[list] = {'training error': []}\n",
    "        self.cache: dict = {}\n",
    "\n",
    "class Backpropagation(TrainingAlgorithm):\n",
    "    def __init__(self, x_data: pd.DataFrame, y_data: pd.DataFrame, network: NeuralNetwork,\n",
    "                 learning_rate: float,\n",
    "                 error_function: ErrorFunction, stopping_condition: StoppingCondition,\n",
    "                 regularization_term: RegularizationTerm = None, batch_size: int = None\n",
    "                 ):\n",
    "        super().__init__(x_data, y_data, network)\n",
    "        self.learning_rate: float = learning_rate\n",
    "        self.err_fun: ErrorFunction = error_function; self.stop_cond: StoppingCondition = stopping_condition\n",
    "\n",
    "\n",
    "        # The gradients of the loss function (= error function + regularization term) with respect to the network weights and biases.\n",
    "        # Used for updating the network parameters after each training epoch.\n",
    "        self.weights_gradient: ListOfArrays = None; self.biases_gradient: ListOfArrays = None\n",
    "        \n",
    "\n",
    "        \n",
    "        self.regularization_term: RegularizationTerm = regularization_term or NoRegularization()\n",
    "        self.regularization_term.set_network(self.network)\n",
    "\n",
    "        self.batch_size: int = batch_size or len(self.x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.history['weights'] = []\n",
    "        self.history['biases'] = []\n",
    "\n",
    "\n",
    "    def compute_loss(self, x: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Computes the loss of the current network over the given data.\n",
    "        Such data may be the whole training data (self.x, self.y) or may be a mini-batch.\n",
    "\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: np.ndarray\n",
    "            The input training (mini)batch data.\n",
    "        y: np.ndarray\n",
    "            The output training (mini)batch data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "            Training error + Regularization penalty term. \n",
    "        \"\"\"\n",
    "        return self.compute_training_error(x, y) + self.regularization_term()\n",
    "    \n",
    "    def compute_training_error(self, x: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Computes the error of the current network over the given data.\n",
    "        Such data may be the whole training data (self.x, self.y) or may be a mini-batch.\n",
    "        Such error includes no regularization penalty term.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: np.ndarray\n",
    "            The input training (mini)batch data.\n",
    "        y: np.ndarray\n",
    "            The output training (mini)batch data.        \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        error: float\n",
    "            Average of single-pattern errors over the dataset. \n",
    "        \"\"\"\n",
    "        network_outputs: np.ndarray = self.network.compute_multiple_outputs(x)\n",
    "        return self.err_fun(y, network_outputs)\n",
    "    \n",
    "    def update_network_parameters(self, current_minibatch_size: int) -> None:\n",
    "        \"\"\"\n",
    "        NOTE TO SELF: add the momentum term's contribution here later.\n",
    "        Updates the weights and biases of the NeuralNetwork based on the computed gradients and the learning rate.\n",
    "        \"\"\"\n",
    "        factor: float = self.learning_rate * current_minibatch_size / len(self.x)\n",
    "        # Note that weights and biases are stored in each individual layer, so an iteration over\n",
    "        # layers is necessary. NeuralNetwork has no weights: ListOfArrays attribute, so we can't just\n",
    "        # write self.network.weights += ... * self.weights_gradient\n",
    "        for i, l in enumerate(self.network.layers_with_weights):\n",
    "            l.weights += factor * self.weights_gradient[i]   \n",
    "            l.biases += factor * self.biases_gradient[i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.history['weights'] += [ListOfArrays([\n",
    "            l.weights for l in self.network.layers_with_weights\n",
    "        ])]\n",
    "        self.history['biases'] += [ListOfArrays([\n",
    "            l.biases for l in self.network.layers_with_weights\n",
    "        ])]\n",
    "    \n",
    "    def run(self, max_epochs: int) -> None:\n",
    "        \"\"\"\n",
    "        Train the NeuralNetwork according to this strategy.\n",
    "        \"\"\"\n",
    "        epoch: int = 0; tr_err: float = float('inf')\n",
    "        indices = np.arange(len(self.x))\n",
    "        while not self.stop_cond.is_satisfied(tr_err, None) and epoch < max_epochs:\n",
    "            epoch += 1; tr_err = 0\n",
    "\n",
    "            # Shuffle the data\n",
    "            np.random.shuffle(indices)\n",
    "            shuffled_x = self.x[indices]; shuffled_y = self.y[indices]\n",
    "\n",
    "            for start in range(0, len(self.x), self.batch_size):  # For loop over the minibatches.\n",
    "                end = start + self.batch_size\n",
    "                minibatch_x = shuffled_x[start:end]; minibatch_y = shuffled_y[start:end]\n",
    "                current_minibatch_size = len(minibatch_x)\n",
    "\n",
    "                self.update_gradients(minibatch_x, minibatch_y, cache_error = True)  # NOTE define this function.\n",
    "                tr_err += self.cache['minibatch_training_error'] * current_minibatch_size / len(self.x)  # Add the error of the minibatch to the whole-batch error.\n",
    "                self.update_network_parameters(current_minibatch_size)\n",
    "\n",
    "\n",
    "            self.history['training error'] += [tr_err]\n",
    "    \n",
    "    def update_gradients(self, x, y, cache_error: bool) -> None:\n",
    "        # Set gradients to 0.\n",
    "        self.reset_gradients()\n",
    "        if cache_error: self.cache['minibatch_training_error'] = 0\n",
    "        # Set gradients to the mean of single-pattern gradients of the error (without regularization) function.\n",
    "        for pattern_x, pattern_y in zip(x,y):\n",
    "            self.add_single_pattern_gradients_contribution(pattern_x, pattern_y, cache_error)\n",
    "        current_minibatch_size: int = len(x)\n",
    "        self.weights_gradient /= current_minibatch_size; self.biases_gradient /= current_minibatch_size\n",
    "        if cache_error: self.cache['minibatch_training_error'] /= current_minibatch_size\n",
    "        # Add the regularization term contribution.\n",
    "        w_reg_term, b_reg_term = self.regularization_term.gradient()\n",
    "        self.weights_gradient += w_reg_term; self.biases_gradient += b_reg_term\n",
    "    \n",
    "    def reset_gradients(self) -> None:\n",
    "        \"\"\"\n",
    "        Sets self.weights_gradient and self.biases_gradient to a list of appropriately-shaped arrays with all zero entries.\n",
    "        \"\"\"\n",
    "        lwws: list[Layer] = self.network.layers_with_weights\n",
    "        self.weights_gradient = ListOfArrays([np.zeros_like(l.weights) for l in lwws])\n",
    "        self.biases_gradient = ListOfArrays([np.zeros_like(l.biases) for l in lwws])\n",
    "\n",
    "    def add_single_pattern_gradients_contribution(self, pattern_x: np.ndarray, pattern_y: np.ndarray, cache_error: bool) -> None:\n",
    "        predicted_y: np.ndarray = self.network.compute_output(pattern_x)\n",
    "        if cache_error: self.cache['minibatch_training_error'] += self.err_fun(pattern_y, predicted_y)\n",
    "        self.backpropagate(pattern_y)\n",
    "\n",
    "        w_term = ListOfArrays([np.outer(l.previous_layer.output, l.delta) for l in self.network.layers_with_weights])\n",
    "        b_term = ListOfArrays([l.delta for l in self.network.layers_with_weights])\n",
    "\n",
    "        self.weights_gradient += w_term; self.biases_gradient += b_term\n",
    "    \n",
    "    def backpropagate(self, pattern_y: np.ndarray) -> None:\n",
    "        out_l: OutputLayer = self.network.output_layer; hid_ls: list[HiddenLayer] = self.network.hidden_layers  # Give short, convenient names to the OutputLayer and the HiddenLayers.\n",
    "\n",
    "        out_l.delta = out_l.activation_function.derivative(out_l.linear_output) * self.err_fun.simple_gradient(pattern_y, out_l.output)  # Compute delta for the OutputLayer.\n",
    "        for l in reversed(hid_ls):  # For each HiddenLayer, starting from the one closest to the OutputLayer and proceeding backwards:\n",
    "            l.delta = l.activation_function.derivative(l.linear_output) * np.dot(l.next_layer.weights, l.next_layer.delta)  # Compute delta for the HiddenLayer.\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [InputLayer(2), HiddenLayer(3, RandomUniform(0.0), Sigmoid()), OutputLayer(2, RandomUniform(0.0), Sigmoid())]\n",
    "nn = NeuralNetwork(layers)\n",
    "\n",
    "# Initialize layers\n",
    "nn.hidden_layers[0].weights = np.array(\n",
    "    [[1, 0.5, 2],\n",
    "    [-1, 0, -1.5]], dtype = float\n",
    ")\n",
    "nn.hidden_layers[0].biases = np.array([1, 2, 3], dtype=float)\n",
    "\n",
    "nn.output_layer.weights = np.array(\n",
    "    [[1, 3],\n",
    "     [2, 3],\n",
    "     [-1, 0]], dtype = float\n",
    ")\n",
    "nn.output_layer.biases = np.array([0,2], dtype=float)\n",
    "\n",
    "\n",
    "# Some fake training data, now with more data\n",
    "x = np.array(\n",
    "    [[1, 0],\n",
    "     [1, 0],\n",
    "     [-1, 3]], dtype = float\n",
    ")\n",
    "y = np.array(\n",
    "    [[1, 0],\n",
    "     [1, 0],\n",
    "     [2, 3]], dtype = float\n",
    ")\n",
    "\n",
    "training_alg = Backpropagation(x, y, nn, 200.0, MSE(), ThresholdOnLoss(0.00001, 10), regularization_term = Tikhonov(0.000), batch_size = 2)\n",
    "\n",
    "old_weights = ListOfArrays([np.copy(nn.hidden_layers[0].weights), np.copy(nn.output_layer.weights)])\n",
    "old_biases = ListOfArrays([np.copy(nn.hidden_layers[0].biases), np.copy(nn.output_layer.biases)])\n",
    "\n",
    "np.random.seed(1)\n",
    "training_alg.run(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_weights = training_alg.history['weights'][0]\n",
    "new_weights = training_alg.history['weights'][1]\n",
    "\n",
    "intermediate_biases = training_alg.history['biases'][0]\n",
    "new_biases = training_alg.history['biases'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListOfArrays[array([[-9.27071293e-04, -7.44532939e-04,  2.19349024e-08],\n",
      "       [ 0.00000000e+00,  1.77635684e-15, -1.11022302e-16]]), array([[ 0.00000000e+00, -3.18824173e-03],\n",
      "       [ 1.77635684e-15, -3.33229203e-03],\n",
      "       [ 0.00000000e+00, -3.50297351e-03]])]\n"
     ]
    }
   ],
   "source": [
    "print((intermediate_weights - old_weights)\n",
    "      -\n",
    "      ( tensor_prod_for_1010 * 1/3 + tensor_prod_for_m1323 * 1/3)*training_alg.learning_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_nn = NeuralNetwork([\n",
    "    InputLayer(2),\n",
    "    HiddenLayer(3, RandomUniform(0), activation_function=Sigmoid()),\n",
    "    OutputLayer(2, RandomUniform(0), activation_function=Sigmoid())\n",
    "])\n",
    "for i, l in enumerate(intermediate_nn.layers_with_weights):\n",
    "    l.weights = intermediate_weights[i]\n",
    "    l.biases = intermediate_biases[i]\n",
    "\n",
    "\n",
    "training_alg2 = Backpropagation(x, y, intermediate_nn, 200.0, MSE(), ThresholdOnLoss(0.00001, 10), regularization_term = Tikhonov(0.000), batch_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_nn.compute_output(x[2])\n",
    "\n",
    "training_alg2.backpropagate(y[2])\n",
    "\n",
    "intermediate_tensors = ListOfArrays([\n",
    "    np.outer(\n",
    "        l.previous_layer.output, l.delta\n",
    "    ) for l in intermediate_nn.layers_with_weights\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListOfArrays[array([[ 2.03390184e-04,  0.00000000e+00, -2.26760412e-08],\n",
       "       [-6.10170553e-04,  0.00000000e+00,  6.80281237e-08]]), array([[-3.38358582e-10, -4.39081919e-03],\n",
       "       [-3.43621428e-10, -4.45911420e-03],\n",
       "       [-3.95538681e-13, -5.13283518e-06]])]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_weights - intermediate_weights - (intermediate_tensors / 3)*training_alg.learning_rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
